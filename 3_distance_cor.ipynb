{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear dependencies amongst the SDGs and climate change by distance correlation\n",
    "\n",
    "We start with investigating dependencies amongst the SDGs on different levels. The method how we investigate these dependencies should take as few assumptions as possible. So, a Pearson linear correlation coefficient or a rank correlation coefficient are not our choice since they assume linearity and/or monotony, respectively.\n",
    "\n",
    "We choose to compute the [distance correlation](https://projecteuclid.org/euclid.aos/1201012979), precisely the [partial distance correlation](https://projecteuclid.org/download/pdfview_1/euclid.aos/1413810731), because of the following properties:\n",
    "1. we have an absolute measure of dependence ranging from $0$ to $1$, $0 \\leq \\mathcal{R}(X,Y) \\leq 1$\n",
    "2. $\\mathcal{R}(X,Y) = 0$ if and only if $X$ and $Y$ are independent,\n",
    "3. $\\mathcal{R}(X,Y) = \\mathcal{R}(Y,X)$\n",
    "4. we are able to investigate non-linear and non-monotone relationships,\n",
    "5. we can find dependencies between indicators with differently many measurements,\n",
    "6. the only assumptions we need to take is that probability distributions have finite first moments.\n",
    "\n",
    "The conditional distance correlation has the advantage that we ignore the influence of any other targets or goals when we compute the correlation between any two targets or goals. This procedure is also called controlling for confounders.\n",
    "\n",
    "The **distance correlation** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}^2(X,Y) = \\begin{cases}\n",
    "\\frac{\\mathcal{V}^2 (X,Y)}{\\sqrt{\\mathcal{V}^2 (X)\\mathcal{V}^2 (Y)}} &\\text{, if $\\mathcal{V}^2 (X)\\mathcal{V}^2 (Y) > 0$} \\\\\n",
    "0 &\\text{, if $\\mathcal{V}^2 (X)\\mathcal{V}^2 (Y) = 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{V}^2 (X,Y) = \\| f_{X,Y}(t) - f_X(t)f_Y(t) \\|^2\n",
    "$$\n",
    "\n",
    "\n",
    "is the distance covariance with **characteristic functions** $f(t)$. Bear in mind that characteristic functions include the imaginary unit $i$, $i^2 = -1$:\n",
    "\n",
    "$$\n",
    "f_X(t) = \\mathbb{E}[e^{itX}]\n",
    "$$\n",
    "\n",
    "Thus, we are in the space of complex numbers $\\mathbb{C}$. Unfortunately, this means we can most likely not find exact results, but we'll get back to this later under Estimators.\n",
    "\n",
    "The **conditional distance correlation** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}^2(X,Y \\ | \\ Z) = \\begin{cases}\n",
    "\\frac{\\mathcal{R}^2 (X,Y) - \\mathcal{R}^2 (X,Z) \\mathcal{R}^2 (Y,Z)}{\\sqrt{1 - \\mathcal{R}^4 (X,Z)} \\sqrt{1 - \\mathcal{R}^4 (Y,Z)}} &\\text{, if $\\mathcal{R}^4 (X,Z) \\neq 1$ and $\\mathcal{R}^4 (Y,Z) \\neq 1$} \\\\\n",
    "0 &\\text{, if $\\mathcal{R}^4 (X,Z) = 1$ and $\\mathcal{R}^4 (Y,Z) = 1$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "# Distance covariance\n",
    "Let's dismantle the distance covariance equation to know what we actually compute in the distance correlation:\n",
    "\n",
    "$$\n",
    "\\mathcal{V}^2 (X,Y) = \\| f_{X,Y}(t) - f_X(t) \\ f_Y(t) \\|^2 = \\frac{1}{c_p c_q} \\int_{\\mathbb{R}^{p+q}} \\frac{| f_{X,Y}(t) - f_X(t)f_Y(t) |^2}{| t |_p^{1+p} \\ | t |_q^{1+q}} dt\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "c_d = \\frac{\\pi^{(1+d)/2}}{\\Gamma \\Big( (1+d)/2 \\Big)}\n",
    "$$\n",
    "\n",
    "where the (complete) Gamma function $\\Gamma$ is\n",
    "\n",
    "$$\n",
    "\\Gamma (z) = \\int_0^{\\infty} x^{z-1} \\ e^{-x} \\ dx\n",
    "$$\n",
    "\n",
    "with $z \\in \\mathbb{R}^{+}$. \n",
    "\n",
    "$p$ and $q$ are the samples of time-series. We can see this as a random vector with multiple samples available for each time point. However, the number of samples for time points must not vary over the same time-series. We can write this as: \n",
    "\n",
    "$$X \\ \\text{in} \\ \\mathbb{R}^p$$\n",
    "\n",
    "$$Y \\ \\text{in} \\ \\mathbb{R}^q$$\n",
    "\n",
    "\n",
    "A preliminary conclusion of this formulation: **we can compute dependencies between time-series with different numbers of samples**. \n",
    "\n",
    "But we still have some terms in the distance covariance $\\mathcal{V}^2 (X,Y)$ which we need to define:\n",
    "\n",
    "$ | t |_p^{1+p} $ is the Euclidean distance of $t$ in $\\mathbb{R}^p$, $ | t |_q^{1+q} $ is the Euclidean distance of $t$ in $\\mathbb{R}^q$.\n",
    "\n",
    "The numerator in the integral of $\\mathcal{V}^2 (X,Y)$ is:\n",
    "$$\n",
    "| f_{X,Y}(t) - f_X(t) \\ f_Y(t) |^2 = \\Big( 1- |f_X(t) | ^2 \\Big) \\ \\Big( 1- |f_Y(t) |^2 \\Big)\n",
    "$$\n",
    "\n",
    "where $|f_X(t) |$ and $|f_Y(t) |$ are absolute random vectors of the characteristic functions $f(t)$ with $p$ and $q$ samples, respectively.\n",
    "\n",
    "\n",
    "## Estimators\n",
    "\n",
    "Since the characteristic functions include the imaginary unit $i$, we cannot recover the exact solution for the distance covariance. However, we can estimate it by a quite simple form. We compute these estimators according to [Huo & Szekely, 2016](https://arxiv.org/abs/1410.1503).\n",
    "\n",
    "We denote the pairwise distances of the $X$ observations by $a_{ij} := \\|X_i - X_j \\|$ and of the $Y$ observations by $b_{ij} = \\|Y_i - Y_j \\|$ for $i,j = 1, ..., n$, where $n$ is the number of measurements in $X$ and $Y$. The corresponding distance matrices are denoted by $(A_{ij})^n_{i,j=1}$ and $(B_{ij})^n_{i,j=1}$, where\n",
    "\n",
    "$$\n",
    "A_{ij} = \\begin{cases}\n",
    "a_{ij} - \\frac{1}{n} \\sum_{l=1}^n a_{il} - \\frac{1}{n} \\sum_{k=1}^n a_{kj} + \\frac{1}{n^2} \\sum_{k,l=1}^n a_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "B_{ij} = \\begin{cases}\n",
    "b_{ij} - \\frac{1}{n} \\sum_{l=1}^n b_{il} - \\frac{1}{n} \\sum_{k=1}^n b_{kj} + \\frac{1}{n^2} \\sum_{k,l=1}^n b_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Having computed these, we can estimate the sample distance covariance $\\hat{\\mathcal{V}}^2(X,Y)$ by\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{V}}^2(X,Y) = \\frac{1}{n^2} \\sum_{i,j=1}^n A_{ij} \\ B_{ij}\n",
    "$$\n",
    "\n",
    "The corresponding sample variance $\\hat{\\mathcal{V}}^2(X)$ is consequently:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{V}}^2(X) = \\frac{1}{n^2} \\sum_{i,j=1}^n A^2_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, we can scale these covariances to finally arrive at the sample distance correlation $\\hat{\\mathcal{R}}^2(X,Y)$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{R}}^2(X,Y) = \\begin{cases}\n",
    "\\frac{\\hat{\\mathcal{V}}^2 (X,Y)}{\\sqrt{\\hat{\\mathcal{V}}^2 (X)\\hat{\\mathcal{V}}^2 (Y)}} &\\text{, if $\\hat{\\mathcal{V}}^2 (X)\\mathcal{V}^2 (Y) > 0$} \\\\\n",
    "0 &\\text{, if $\\hat{\\mathcal{V}}^2 (X)\\hat{\\mathcal{V}}^2 (Y) = 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Unbiased estimators\n",
    "These estimators are biased, but we can define unbiased estimators of the distance covariance $\\hat{\\mathcal{V}}^2(X,Y)$ and call them $\\Omega_n(x,y)$. We must first redefine our distance matrices $(A_{ij})^n_{i,j=1}$ and $(B_{ij})^n_{i,j=1}$, which we will call $(\\tilde{A}_{ij})^n_{i,j=1}$ and $(\\tilde{B}_{ij})^n_{i,j=1}$:\n",
    "\n",
    "$$\n",
    "\\tilde{A}_{ij} = \\begin{cases}\n",
    "a_{ij} - \\frac{1}{n-2} \\sum_{l=1}^n a_{il} - \\frac{1}{n-2} \\sum_{k=1}^n a_{kj} + \\frac{1}{(n-1)(n-2)} \\sum_{k,l=1}^n a_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\tilde{B}_{ij} = \\begin{cases}\n",
    "b_{ij} - \\frac{1}{n-2} \\sum_{l=1}^n b_{il} - \\frac{1}{n-2} \\sum_{k=1}^n b_{kj} + \\frac{1}{(n-1)(n-2)} \\sum_{k,l=1}^n b_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally, we can compute the unbiased estimator $\\Omega_n(X,Y)$ for $\\mathcal{V}^2(X,Y)$ as the dot product $\\langle \\tilde{A}, \\tilde{B} \\rangle$:\n",
    "\n",
    "$$\n",
    "\\Omega_n(X,Y) = \\langle \\tilde{A}, \\tilde{B} \\rangle = \\frac{1}{n(n-3)} \\sum_{i,j=1}^n \\tilde{A}_{ij} \\ \\tilde{B}_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "Interestingly, [Lyons (2013)](https://arxiv.org/abs/1106.5758) found another solution how not only the sample distance correlation can be computed, but also the population distance correlation without characteristic functions. This is good to acknowledge, but it is not necessary to focus on it. \n",
    "\n",
    "# Conditional distance covariance\n",
    "\n",
    "We start with computing the unbiased distance matrices $(\\tilde{A}_{ij})^n_{i,j=1}$, $(\\tilde{B}_{ij})^n_{i,j=1}$, and $(\\tilde{C}_{ij})^n_{i,j=1}$ for $X$, $Y$, and $Z$, respectively, as we have done previously for the distance covariance. We define the dot product\n",
    "\n",
    "$$\n",
    "\\Omega_n(X,Y) = \\langle \\tilde{A}, \\tilde{B} \\rangle = \\frac{1}{n(n-3)} \\sum_{i,j=1}^n \\tilde{A}_{ij} \\tilde{B}_{ij}\n",
    "$$\n",
    "\n",
    "and project the sample $x$ onto $z$ as \n",
    "\n",
    "$$\n",
    "P_z (x) = \\frac{\\langle \\tilde{A}, \\tilde{C} \\rangle}{\\langle \\tilde{C}, \\tilde{C} \\rangle} \\tilde{C} .\n",
    "$$\n",
    "\n",
    "The complementary projection is consequently\n",
    "\n",
    "$$\n",
    "P_{z^{\\bot}} (x) = \\tilde{A} - P_z (x) = \\tilde{A} - \\frac{\\langle \\tilde{A}, \\tilde{C} \\rangle}{\\langle \\tilde{C}, \\tilde{C} \\rangle} \\tilde{C} .\n",
    "$$\n",
    "\n",
    "Hence, the sample conditional distance covariance is\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{V}}^2(X,Y \\ | \\ Z) = \\langle P_{z^{\\bot}} (x), P_{z^{\\bot}} (y) \\rangle .\n",
    "$$\n",
    "\n",
    "Then, we can scale these covariances to finally arrive at the sample conditional distance correlation $\\hat{\\mathcal{R}}^2(X,Y \\ | \\ Z)$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{R}}^2(X,Y \\ | \\ Z) = \\begin{cases}\n",
    "\\frac{\\langle P_{z^{\\bot}} (x), P_{z^{\\bot}} (y) \\rangle}{\\| P_{z^{\\bot}} (x) \\| \\ \\| P_{z^{\\bot}} (y) \\|} &\\text{, if} \\ \\| P_{z^{\\bot}} (x) \\| \\ \\| P_{z^{\\bot}} (y) \\| \\neq 0 \\\\\n",
    "0 &\\text{, if} \\ \\| P_{z^{\\bot}} (x) \\| \\ \\| P_{z^{\\bot}} (y) \\| = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Implementation\n",
    "For our computations, we'll use the packages [`dcor`](https://dcor.readthedocs.io/en/latest/?badge=latest) for the partial distance correlation and [`community`](https://github.com/taynaud/python-louvain) for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dcor\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import community\n",
    "\n",
    "from dcor._dcor_internals import _u_distance_matrix, u_complementary_projection\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading standardised imputed data set\n",
    "We load first of all the standardised imputed data set which we have generated with the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_values_i = pickle.load(open('utils/data/indicators_values_i_up_wb.pkl', 'rb'))\n",
    "targets_values_i = pickle.load(open('utils/data/targets_values_i_up_wb.pkl', 'rb'))\n",
    "goals_values_i = pickle.load(open('utils/data/goals_values_i_up_wb.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether T appended\n",
    "targets_values_i['Belgium'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our temperature data is available from 1991 to 2016 only. Additionally, SDG 13 has data from 2005 to 2019 only. Thus, we can only compute the distance covariances for the 12-dimensional random vectors from 2005 to 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = ['2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read amended csv file\n",
    "c = pd.read_csv('utils/countries_wb.csv', dtype=str, delimiter=';', header=None)\n",
    "countries = list(c[0])\n",
    "continents = pd.read_csv(r'utils/continents.csv')\n",
    "groups = pd.read_csv(r'utils/groups.csv')\n",
    "groups.replace({\"Democratic People's Republic of Korea\": \"Korea, Dem. People's Rep.\", 'Gambia': 'Gambia, The', 'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom', 'Congo': 'Congo, Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.', 'Czechia': 'Czech Republic', 'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'Kyrgyzstan': 'Kyrgyz Republic', \"Lao People's Democratic Republic\": 'Lao PDR', 'Republic of Moldova': 'Moldova', 'Micronesia (Federated States of)': 'Micronesia, Fed. Sts.', 'Slovakia': 'Slovak Republic', 'Viet Nam': 'Vietnam', 'Egypt': 'Egypt, Arab Rep.', 'United Republic of Tanzania': 'Tanzania','United States of America': 'United States', 'Venezuela (Bolivarian Republic of)': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.', 'Bahamas': 'Bahamas, The', 'Bolivia (Plurinational State of)': 'Bolivia'}, inplace=True)\n",
    "continents.replace({\"Democratic People's Republic of Korea\": \"Korea, Dem. People's Rep.\", 'Gambia': 'Gambia, The', 'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom', 'Congo': 'Congo, Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.', 'Czechia': 'Czech Republic', 'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'Kyrgyzstan': 'Kyrgyz Republic', \"Lao People's Democratic Republic\": 'Lao PDR', 'Republic of Moldova': 'Moldova', 'Micronesia (Federated States of)': 'Micronesia, Fed. Sts.', 'Slovakia': 'Slovak Republic', 'Viet Nam': 'Vietnam', 'Egypt': 'Egypt, Arab Rep.', 'United Republic of Tanzania': 'Tanzania','United States of America': 'United States', 'Venezuela (Bolivarian Republic of)': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.', 'Bahamas': 'Bahamas, The', 'Bolivia (Plurinational State of)': 'Bolivia'}, inplace=True)\n",
    "info = pd.read_csv(r'utils/wb_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We later compute the correlations on an indicator level, but this is too detailed for any network visualisation and for an overarching understanding. Hence, we group here all sub-indicators first on an indicator-level. Then, we compute the distance correlations for the indicators, targets and goals.\n",
    "\n",
    "We work with the `info` file again, so we don't need to assign all of this by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "info['Series Code'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "targets_values_i['France'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have values for targets, so we must, first of all, generate a list of all unique **targets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(info['target'].unique())    # T for UN data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as with the series codes, we need lists of indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_targets = {}\n",
    "\n",
    "for target in targets:\n",
    "    t = info['Series Code'].where(info['target'] == target)    # replace Series Code with Indicator for UN data set\n",
    "\n",
    "    dict_targets[target] = list(set([i for i in t if str(i) != 'nan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check \n",
    "dict_targets['1.2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we also generate a list of all unique **goals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goals = list(info['SDG'].unique())    # replace SDG with Goal for UN data set\n",
    "goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as above, lists of targets belonging to each goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_goals = {}\n",
    "\n",
    "for goal in goals:\n",
    "    g = info['target'].where(info['SDG'] == goal)\n",
    "\n",
    "    dict_goals[goal] = list(set([t for t in g if str(t) != 'nan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check \n",
    "print(dict_goals['13'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "#### Partial distance covariance interlude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.uniform(-5,5,512).reshape(-1,1)\n",
    "B = A + np.random.uniform(-1,1,512).reshape(-1,1)\n",
    "C = 0.25*A**2 + np.random.uniform(-1,1,512).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(B, C);\n",
    "\n",
    "print('pairwise distance correlation:', dcor.distance_correlation(B, C))\n",
    "print('partial distance correlation:', dcor.partial_distance_correlation(B, C, A))\n",
    "print('Pearson correlation:', np.corrcoef(B, C, rowvar=False)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $B$ and $C$ are pairwise dependent, but $B$ is independent of $C$ given $A$.\n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance correlations between goals\n",
    "\n",
    "The next step is to compute the distance correlations on a goal-level.\n",
    "\n",
    "We work with the **concatenated time-series** to compute the conditioned distance correlation directly on goal-level data. Visually speaking, this means that we fit one non-linear function to the data for all targets of these two goals. Since goals often have diverse targets, this may end up in fitting a non-linear curve to very noisy data.\n",
    "\n",
    "## Working with concatenated time-series\n",
    "\n",
    "### Conditioning iteratively on subsets of joint distributions of all goals\n",
    "We condition pairs of two goals iteratively on subsets of all remaining goals. We start with conditioning on the empty set, i.e. we compute the pairwise distance correlation first. Afterwards, we increase the set to condition on until we have reached the set of all remaining 15 goals to condition on. These sets are represented by the joint distributions of the goals entailed in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to condition on all **subsets** of these lists of SDGs we condition on to find the dependence which solely stems from either of the two SDGs we condition the others on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(iterable, r):\n",
    "    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "    # combinations(range(4), 3) --> 012 013 023 123\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield list(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i+1, r):\n",
    "            indices[j] = indices[j-1] + 1\n",
    "        yield list(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_tuple(iterable, r):\n",
    "    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "    # combinations(range(4), 3) --> 012 013 023 123\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield tuple(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i+1, r):\n",
    "            indices[j] = indices[j-1] + 1\n",
    "        yield tuple(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(pool_0, pool_1):\n",
    "    #result = [[x, y]+[z] for x, y in pool_0 for z in pool_1 if x not in z and y not in z]    # ~ 10 Mio rows\n",
    "    result = [[x, y]+[z] for x, y in pool_0 for z in pool_1]    # ~ 40 Mio rows\n",
    "    for prod in result:\n",
    "        yield tuple(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list out of all unique combinations of goals\n",
    "g_combinations = list(combinations(goals, 2))\n",
    "conditions_g = []\n",
    "conditions_g_tuple = []\n",
    "for i in range(1, 18):\n",
    "    conditions_g.extend(list(combinations(goals, i)))\n",
    "    conditions_g_tuple.extend(tuple(combinations_tuple(goals, i)))\n",
    "\n",
    "pairs = list(product(g_combinations, conditions_g))\n",
    "pairs_g0 = pd.DataFrame.from_records(pairs, columns=['pair_0', 'pair_1', 'condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding empty condition set for pairwise dcor\n",
    "pairs_g1 = pd.DataFrame.from_records(data=g_combinations, columns=['pair_0', 'pair_1'])\n",
    "pairs_g1['condition'] = 0\n",
    "pairs_g = pd.concat([pairs_g0, pairs_g1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "pairs_g.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes\n",
    "print(len(conditions_g))\n",
    "print(pairs_g0.shape)\n",
    "print(pairs_g1.shape)\n",
    "print(pairs_g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dcor computations, we need the same number of samples in each year. We bootstrap the under-represented years to the same number of samples of the year with the maximum samples. We need bootstrapped data for all goals, so we can call them afterwards when conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "goals_values_i['France']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "dict_cov_goals_continents_2 = pickle.load(open('distance_cor/goals/dict_cov_goals_continents_2.pkl', 'rb'))\n",
    "dict_cor_goals_continents_2 = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "continents_prep_g = {}\n",
    "continents_prep_boot_g = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    continents_prep_g[continent] = pd.DataFrame(columns=period, index=goals, dtype=object)\n",
    "    continents_prep_boot_g[continent] = pd.DataFrame(columns=period, index=goals, dtype=object)\n",
    "    \n",
    "    for goal in goals:\n",
    "        max_year_g = 0\n",
    "        \n",
    "        for year in period:\n",
    "            y_g = []\n",
    "\n",
    "            for country in continents[continent].dropna():\n",
    "                y_g.extend(goals_values_i[country].loc[str(goal), year])\n",
    "            \n",
    "            continents_prep_g[continent].loc[goal, year] = np.array(y_g)\n",
    "\n",
    "            # finding year with most measurements\n",
    "            len_year_g = len(y_g)\n",
    "            if len_year_g > max_year_g:\n",
    "                max_year_g = len_year_g\n",
    "                \n",
    "        # bootstrap to have same number of samples in each year\n",
    "        for year in period:\n",
    "            # not considering the years without samples\n",
    "            if len(continents_prep_g[continent].loc[goal, year]) == 0:\n",
    "                continue    \n",
    "            else:\n",
    "                continents_prep_boot_g[continent].loc[goal, year] = np.random.choice(continents_prep_g[continent].loc[goal, year], size=max_year_g, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call these data in our `dcor` computations. We first compute the pairwise distance covariance and correlation, then the partial ones with conditioning on all the previously defined sets in `pairs_g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "Filtering out the conditions that contain goals already being $X$ (`pair_0`) or $Y$ (`pair_1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # repeating conditions 153 times to bring to same length as pair_0 and pair_1\n",
    "conditions_g_tuple_all = conditions_g_tuple * len(pairs_g1)\n",
    "conditions_g_all = conditions_g * len(pairs_g1)\n",
    "\n",
    "# create lists which include rows that do not appear in both pair_0 or pair_1 and conditions\n",
    "pair_0 = pairs_g0['pair_0'].to_list()\n",
    "pair_1 = pairs_g0['pair_1'].to_list()\n",
    "\n",
    "idx_to_delete = []\n",
    "idx_to_keep = []\n",
    "\n",
    "for c in tqdm.tqdm(range(len(pairs_g1))):\n",
    "    for i in range(len(conditions_g)):\n",
    "        if pair_0[i*c] in conditions_g_all[i*c]:\n",
    "            idx_to_delete.append(i*c)\n",
    "        elif pair_1[i*c] in conditions_g_all[i*c]:\n",
    "            idx_to_delete.append(i*c)\n",
    "        else:\n",
    "            idx_to_keep.append(i*c)    # which rows to keep\n",
    "\n",
    "# deleting rows with same goals in pairs_0 or pairs_1 and condition\n",
    "pair_0_left = list(itertools.compress(pair_0, idx_to_keep))\n",
    "pair_1_left = list(itertools.compress(pair_1, idx_to_keep))\n",
    "conditions_tuple_left = list(itertools.compress(conditions_g_tuple_all, idx_to_keep))\n",
    "conditions_left = list(itertools.compress(conditions_g_all, idx_to_keep))\n",
    "pairs_g_left = pd.DataFrame.from_dict({'pair_0': pair_0_left, 'pair_1': pair_1_left, 'condition': conditions_tuple_left})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without parallelisation\n",
    "\n",
    "*(don't even try running it, takes 800 hours per continent)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "# have conditions in joint random vector\n",
    "dict_cov_goals_continents_2 = {}\n",
    "dict_cor_goals_continents_2 = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    dict_cov_goa_c = pairs_g_left.copy()\n",
    "    dict_cor_goa_c = pairs_g_left.copy()\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(pairs_g_left))):\n",
    "        \n",
    "        # pairwise distance correlation\n",
    "        if pairs_g_left.iloc[i]['condition'] == 0:\n",
    "            dict_cov_goa_c.loc[i, 'dcov'] = dcor.distance_covariance(np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_0']].to_list()), np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_1']].to_list()))**2\n",
    "            dict_cor_goa_c.loc[i, 'dcor'] = dcor.distance_correlation(np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_0']].to_list()), np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_1']].to_list()))**2\n",
    "\n",
    "        # partial distance correlation\n",
    "        else:\n",
    "            # build conditional set\n",
    "            condition = pd.DataFrame(index=period, columns=pairs_g_left.iloc[i]['condition'] + ['combined'])\n",
    "            for y in period:\n",
    "                condition.loc[y, 'combined'] = []\n",
    "            for c in pairs_g_left.iloc[i]['condition']:\n",
    "                condition[c] = continents_prep_boot_g[continent].loc[c]\n",
    "                for y in period:\n",
    "                    condition.loc[y, 'combined'].extend(condition.loc[y, c])\n",
    "                \n",
    "                # drop every column except 'combined' to save memory\n",
    "                condition.drop(columns=c, inplace=True)\n",
    "\n",
    "            # partial distance correlation  \n",
    "            dict_cov_goa_c.loc[i, 'dcov'] = dcor.partial_distance_covariance(np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_0']].to_list()), np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_1']].to_list()), np.array(condition['combined'].to_list()))**2\n",
    "            dict_cor_goa_c.loc[i, 'dcor'] = dcor.partial_distance_correlation(np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_0']].to_list()), np.array(continents_prep_boot_g[continent].loc[pairs_g_left.iloc[i]['pair_1']].to_list()), np.array(condition['combined'].to_list()))**2\n",
    "        \n",
    "\n",
    "    # find minimum distance correlation between any two goals\n",
    "    dict_cov_goa_con = dict_cov_goa_c.groupby(['pair_0', 'pair_1'])['dcov'].apply(list).reset_index(name='list_dcov')\n",
    "    dict_cor_goa_con = dict_cor_goa_c.groupby(['pair_0', 'pair_1'])['dcor'].apply(list).reset_index(name='list_dcor')\n",
    "    \n",
    "    for i, row_c in dict_cov_goa_con.iterrows():\n",
    "        dict_cov_goa_con.loc[i, 'min_dcov'] = min(dict_cov_goa_con.loc[i, 'list_dcov'])\n",
    "        dict_cor_goa_con.loc[i, 'min_dcor'] = min(dict_cor_goa_con.loc[i, 'list_dcor'])\n",
    "    \n",
    "    dict_cov_goals_continents_2[continent] = dict_cov_goa_con\n",
    "    dict_cor_goals_continents_2[continent] = dict_cor_goa_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "g_cov = open('distance_cor/goals/dict_cov_goals_continents_2_wp.pkl', 'wb')\n",
    "pickle.dump(dict_cov_goals_continents_2, g_cov)\n",
    "g_cov.close()\n",
    "\n",
    "g_cor = open('distance_cor/goals/dict_cor_goals_continents_2_wp.pkl', 'wb')\n",
    "pickle.dump(dict_cor_goals_continents_2, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With  `multiprocessing`  parallelisation\n",
    "\n",
    "\n",
    " \n",
    "### Partial distance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_distance_cov(i, pair_0, pair_1, cond):\n",
    "    pair_0_array = np.array(continents_prep_boot_g[continent].loc[pair_0].to_list())\n",
    "    pair_1_array = np.array(continents_prep_boot_g[continent].loc[pair_1].to_list())\n",
    "    condition_array = np.array(conditions_df.loc[[cond]])\n",
    "    \n",
    "    return dcor.partial_distance_covariance(pair_0_array, pair_1_array, condition_array)**2\n",
    "\n",
    "def partial_distance_cor(i, pair_0, pair_1, cond):\n",
    "    pair_0_array = np.array(continents_prep_boot_g[continent].loc[pair_0].to_list())\n",
    "    pair_1_array = np.array(continents_prep_boot_g[continent].loc[pair_1].to_list())\n",
    "    condition_array = np.array(conditions_df.loc[[cond]])\n",
    "    \n",
    "    return dcor.partial_distance_correlation(pair_0_array, pair_1_array, condition_array)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "\n",
    "dict_cov_goals_continents_2_cond = {}\n",
    "dict_cor_goals_continents_2_cond = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    dict_cov_goa_c = pairs_g_left.copy(deep=True)     # pairs_g_left has all non-empty conditional sets, pairs_g1 has empty conditional sets for pairwise dcor\n",
    "    dict_cor_goa_c = pairs_g_left.copy(deep=True)\n",
    "    \n",
    "    # preparing conditional set\n",
    "    conditions_dict = {}\n",
    "\n",
    "    for cond in conditions_g_tuple:\n",
    "        conditions = np.empty([len(period), len(cond)], dtype=object)    \n",
    "        conditions_comb = np.empty([len(period), ], dtype=object)\n",
    "        conditions_comb_boot = np.empty([len(period), ], dtype=object)\n",
    "\n",
    "        for i_y, y in enumerate(period):\n",
    "            max_len_year = 0\n",
    "            year = []\n",
    "            for i_c, c in enumerate(cond):\n",
    "                conditions[i_y, i_c] = continents_prep_boot_g[continent].loc[c, y]\n",
    "                year.extend(conditions[i_y, i_c])\n",
    "\n",
    "            # finding year with most measurements\n",
    "            if len(year) > max_len_year:\n",
    "                max_len_year = len(year)\n",
    "\n",
    "            conditions_comb[i_y] = np.array(year)\n",
    "        # bootstrap to have same number of samples in each year\n",
    "        for i_y, y in enumerate(period):\n",
    "            conditions_comb_boot[i_y] = np.random.choice(conditions_comb[i_y], size=max_len_year, replace=True)\n",
    "\n",
    "        conditions_dict[cond] = conditions_comb_boot   # combined\n",
    "\n",
    "    # make dataframe out of dictionary\n",
    "    conditions_df = pd.DataFrame.from_dict(conditions_dict, orient='index', columns=period)\n",
    "    \n",
    "    print('start dcor...')\n",
    "    \n",
    "    # partial distance correlation\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    cov_results = []\n",
    "    cor_results = []\n",
    "    \n",
    "    # pairwise distance correlation\n",
    "    for row in dict_cov_goa_c.itertuples(name=None):\n",
    "        cov_results.append(pool.apply_async(partial_distance_cov, args=row).get())\n",
    "        cor_results.append(pool.apply_async(partial_distance_cor, args=row).get())\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    dict_cov_goa_c['dcov'] = cov_results\n",
    "    dict_cor_goa_c['dcor'] = cor_results\n",
    "    \n",
    "    print('...dcor done')\n",
    "\n",
    "    # find minimum distance correlation between any two goals\n",
    "    dict_cov_goa_con = dict_cov_goa_c.groupby(['pair_0', 'pair_1'])['dcov'].apply(list).reset_index(name='list_dcov')\n",
    "    dict_cor_goa_con = dict_cor_goa_c.groupby(['pair_0', 'pair_1'])['dcor'].apply(list).reset_index(name='list_dcor')\n",
    "    \n",
    "    for i, row_c in dict_cov_goa_con.iterrows():\n",
    "        dict_cov_goa_con.loc[i, 'min_dcov'] = min(dict_cov_goa_con.loc[i, 'list_dcov'])\n",
    "        dict_cor_goa_con.loc[i, 'min_dcor'] = min(dict_cor_goa_con.loc[i, 'list_dcor'])\n",
    "    \n",
    "    dict_cov_goals_continents_2_cond[continent] = dict_cov_goa_con\n",
    "    dict_cor_goals_continents_2_cond[continent] = dict_cor_goa_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dict_cor_goals_continents_2['Northern Africa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_cov(i, pair_0, pair_1):\n",
    "    pair_0_array = np.array(continents_prep_boot_g[continent].loc[pair_0].to_list())\n",
    "    pair_1_array = np.array(continents_prep_boot_g[continent].loc[pair_1].to_list())\n",
    "    \n",
    "    return dcor.distance_covariance(pair_0_array, pair_1_array)**2\n",
    "\n",
    "def distance_cor(i, pair_0, pair_1):\n",
    "    pair_0_array = np.array(continents_prep_boot_g[continent].loc[pair_0].to_list())\n",
    "    pair_1_array = np.array(continents_prep_boot_g[continent].loc[pair_1].to_list())\n",
    "    \n",
    "    return dcor.distance_correlation(pair_0_array, pair_1_array)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "\n",
    "dict_cov_goals_continents_2_pair = {}\n",
    "dict_cor_goals_continents_2_pair = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    dict_cov_goa_c_pair = pairs_g1.drop(columns=['condition']).copy()     # pairs_g1 has empty conditional sets for pairwise dcor\n",
    "    dict_cor_goa_c_pair = pairs_g1.drop(columns=['condition']).copy()\n",
    "    \n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    cov_results = []\n",
    "    cor_results = []\n",
    "    \n",
    "    # pairwise distance correlation\n",
    "    for row in dict_cov_goa_c_pair.itertuples(name=None):\n",
    "        cov_results.append(pool.apply_async(distance_cov, args=row).get())\n",
    "        cor_results.append(pool.apply_async(distance_cor, args=row).get())\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    dict_cov_goa_c_pair['dcov'] = cov_results\n",
    "    dict_cor_goa_c_pair['dcor'] = cor_results\n",
    "    \n",
    "    print('dcor done...')\n",
    "    \n",
    "    # find minimum distance correlation between any two goals\n",
    "    dict_cov_goa_con_pair = dict_cov_goa_c_pair.groupby(['pair_0', 'pair_1'])['dcov'].apply(list).reset_index(name='list_dcov')\n",
    "    dict_cor_goa_con_pair = dict_cor_goa_c_pair.groupby(['pair_0', 'pair_1'])['dcor'].apply(list).reset_index(name='list_dcor')\n",
    "    \n",
    "    for i, row_c in dict_cov_goa_c_pair.iterrows():\n",
    "        dict_cov_goa_con_pair.loc[i, 'min_dcov'] = min(dict_cov_goa_con_pair.loc[i, 'list_dcov'])\n",
    "        dict_cor_goa_con_pair.loc[i, 'min_dcor'] = min(dict_cor_goa_con_pair.loc[i, 'list_dcor'])\n",
    "    \n",
    "    dict_cov_goals_continents_2_pair[continent] = dict_cov_goa_con_pair\n",
    "    dict_cor_goals_continents_2_pair[continent] = dict_cor_goa_con_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dict_cor_goals_continents_2_pair['Northern Africa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dictionaries\n",
    "dict_cov_goals_continents_2 = dict_cov_goals_continents_2_cond.update(dict_cov_goals_continents_2_pair)\n",
    "dict_cor_goals_continents_2 = dict_cor_goals_continents_2_cond.update(dict_cor_goals_continents_2_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "g_cov = open('distance_cor/goals/dict_cov_goals_continents_2.pkl', 'wb')\n",
    "pickle.dump(dict_cov_goals_continents_2, g_cov)\n",
    "g_cov.close()\n",
    "\n",
    "g_cor = open('distance_cor/goals/dict_cor_goals_continents_2.pkl', 'wb')\n",
    "pickle.dump(dict_cor_goals_continents_2, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence tests\n",
    "We want to eliminate spurious distance correlations by performing independence tests of the smallest partial distance correlations. As suggested in [Szekely et al. (2007)](https://projecteuclid.org/download/pdfview_1/euclid.aos/1201012979), we use the permutation test to do so. Note that the partial distance covariance test takes the inner product of the double-centered distance matrices of $\\mathbf{U}$ and $\\mathbf{V}$ as the test statistic. $\\mathbf{U}$ and $\\mathbf{V}$ are the metric multi-dimensional scalings (MDS) of the projection matrices $P_z(x)$ and $P_z(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise distance covariance test\n",
    "def dcov_test(X, Y, permutation, alpha):\n",
    "    \n",
    "    statistic = dcor.distance_covariance(X, Y)\n",
    "\n",
    "    dcov_arr = np.zeros(permutation)\n",
    "\n",
    "    # create permutations by reshuffling random vector Y\n",
    "    for per in range(permutation):\n",
    "        Y_perm = np.random.permutation(Y)\n",
    "        dcov_arr[per] = dcor.distance_covariance(X, Y_perm)\n",
    "\n",
    "    dcov_arr_sort = np.sort(dcov_arr)\n",
    "\n",
    "    # computing 1-alpha threshold\n",
    "    threshold = dcov_arr_sort[round((1-alpha)*permutation)]\n",
    "\n",
    "    if statistic > threshold:\n",
    "        dcov_ = dcor.distance_covariance(X, Y)\n",
    "        dcor_ = dcor.distance_correlation(X, Y)\n",
    "\n",
    "    else:\n",
    "        dcov_ = 0\n",
    "        dcor_ = 0\n",
    "    \n",
    "    return dcov_, dcor_\n",
    "\n",
    "# partial distance covariance test\n",
    "def pdcov_test(X, Y, Z, permutation, alpha):\n",
    "    \n",
    "    a = _u_distance_matrix(X)\n",
    "    b = _u_distance_matrix(Y)\n",
    "    c = _u_distance_matrix(Z)\n",
    "\n",
    "    proj = u_complementary_projection(c)\n",
    "\n",
    "    a_p = proj(a)\n",
    "    b_p = proj(b)\n",
    "\n",
    "    embedding = MDS(n_components=2)\n",
    "    U = embedding.fit_transform(a_p)\n",
    "    V = embedding.fit_transform(b_p)\n",
    "\n",
    "    # test statistic\n",
    "    statistic = dcor.u_product(dcor.double_centered(U), dcor.double_centered(V))\n",
    "\n",
    "    # initiating dcor\n",
    "    dcov_arr = np.zeros(permutation)\n",
    "\n",
    "    # create permutations by reshuffling X\n",
    "    for per in range(permutation):\n",
    "        index_perm = np.random.permutation(X.shape[0])\n",
    "        X_perm = X[index_perm]\n",
    "        a_perm = _u_distance_matrix(X_perm)\n",
    "        a_p_perm = proj(a_perm)\n",
    "        U_perm = embedding.fit_transform(a_p_perm)\n",
    "        dcov_arr[per] = dcor.u_product(dcor.double_centered(U_perm), dcor.double_centered(V))\n",
    "\n",
    "    dcov_arr_sort = np.sort(dcov_arr)\n",
    "\n",
    "    # computing 1-alpha threshold\n",
    "    threshold = dcov_arr_sort[round((1-alpha)*permutation)]\n",
    "\n",
    "    if statistic > threshold:\n",
    "        dcov_ = dcor.partial_distance_covariance(X, Y, Z)\n",
    "        dcor_ = dcor.partial_distance_correlation(X, Y, Z)\n",
    "\n",
    "    else:\n",
    "        dcov_ = 0\n",
    "        dcor_ = 0\n",
    "    \n",
    "    return dcov_, dcor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "alpha = 0.05\n",
    "\n",
    "# have conditions in joint random vector\n",
    "dict_cov_goals_continents_2 = {}\n",
    "dict_cor_goals_continents_2 = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    # finding which set to test for independence\n",
    "    to_test_cov = dict_cov_goa_c.loc[dict_cov_goa_c['dcov'] == dict_cov_goa_con.loc[i, 'min_dcov']]\n",
    "    to_test_cor = dict_cor_goa_c.loc[dict_cor_goa_c['dcor'] == dict_cor_goa_con.loc[i, 'min_dcor']]\n",
    "\n",
    "    # empty conditional set\n",
    "    if to_test_cov['condition'].values[0] == 0:\n",
    "        dict_cov_goa_con.loc[i, 'min_dcov_test'] = dcov_test(np.array(continents_prep_boot_g[continent].loc[to_test_cov['pair_0'].values[0]].to_list()), np.array(continents_prep_boot_g[continent].loc[to_test_cov['pair_1'].values[0]].to_list()), permutation=1000, alpha=alpha)[0]\n",
    "        dict_cor_goa_con.loc[i, 'min_dcor_test'] = dcov_test(np.array(continents_prep_boot_g[continent].loc[to_test_cor['pair_0'].values[0]].to_list()), np.array(continents_prep_boot_g[continent].loc[to_test_cor['pair_1'].values[0]].to_list()), permutation=1000, alpha=alpha)[1]\n",
    "\n",
    "    # non-empty conditional set\n",
    "    else:\n",
    "        dict_cov_goa_con.loc[i, 'min_dcov_test'] = pdcov_test(np.array(continents_prep_boot_g[continent].loc[to_test_cov['pair_0'].values[0]].to_list()), np.array(continents_prep_boot_g[continent].loc[to_test_cov['pair_1'].values[0]].to_list()), np.array(condition_test['combined'].to_list()), permutation=1000, alpha=alpha)[0]\n",
    "        dict_cor_goa_con.loc[i, 'min_dcor_test'] = pdcov_test(np.array(continents_prep_boot_g[continent].loc[to_test_cor['pair_0'].values[0]].to_list()), np.array(continents_prep_boot_g[continent].loc[to_test_cor['pair_1'].values[0]].to_list()), np.array(condition_test['combined'].to_list()), permutation=1000, alpha=alpha)[1]\n",
    "\n",
    "dict_cov_goals_continents_2_tests[continent] = dict_cov_goa_con\n",
    "dict_cor_goals_continents_2_tests[continent] = dict_cor_goa_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "g_cov = open('distance_cor/goals/dict_cov_goals_continents_2_tests.pkl', 'wb')\n",
    "pickle.dump(dict_cov_goals_continents_2_tests, g_cov)\n",
    "g_cov.close()\n",
    "\n",
    "g_cor = open('distance_cor/goals/dict_cor_goals_continents_2_tests.pkl', 'wb')\n",
    "pickle.dump(dict_cor_goals_continents_2_tests, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep the minimum distance correlation of each pair of two goals, pairwise or conditioned on any potential subset.\n",
    "\n",
    "The last step is to insert these values into the right cell in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for continent in continents:\n",
    "    for i, row in dict_cov_goals_continents_2[continent].iterrows():\n",
    "        dict_cov_goals_continents_2[continent].loc[i, 'max_dcov'] = max(dict_cov_goals_continents_2[continent].loc[i, 'list_dcov'])\n",
    "        dict_cor_goals_continents_2[continent].loc[i, 'max_dcor'] = max(dict_cor_goals_continents_2[continent].loc[i, 'list_dcor'])\n",
    "        dict_cov_goals_continents_2[continent].loc[i, 'avg_dcov'] = np.mean(dict_cov_goals_continents_2[continent].loc[i, 'list_dcov'])\n",
    "        dict_cor_goals_continents_2[continent].loc[i, 'avg_dcor'] = np.mean(dict_cor_goals_continents_2[continent].loc[i, 'list_dcor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_goals_continents_2 = {}\n",
    "cor_goals_continents_2 = {}\n",
    "\n",
    "for continent in continents:\n",
    "    cov_goals_continents_2[continent] = pd.DataFrame(index=goals, columns=goals)\n",
    "    cor_goals_continents_2[continent] = pd.DataFrame(index=goals, columns=goals)\n",
    "\n",
    "    for i in tqdm(list(dict_cov_goals_continents_2[continent].index)):\n",
    "        goal_0 = dict_cov_goals_continents_2[continent].loc[i, 'pair_0']\n",
    "        goal_1 = dict_cov_goals_continents_2[continent].loc[i, 'pair_1']\n",
    "        \n",
    "        cov_goals_continents_2[continent].loc[goal_1, goal_0] = np.sqrt(dict_cov_goals_continents_2[continent].loc[i, 'min_dcov_test'])\n",
    "        cor_goals_continents_2[continent].loc[goal_1, goal_0] = np.sqrt(dict_cor_goals_continents_2[continent].loc[i, 'min_dcor_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check\n",
    "cor_goals_continents_2['Northern Africa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cor_goals_continents_2` are the conditional distance correlations for all continents in a setting of 18 random vectors $X$, $Y$, and $Z_1, Z_2, ..., Z_{16}$, where $\\boldsymbol{Z}$ is the array containing all random vectors we want to condition on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "if not os.path.exists('distance_cor/goals'):\n",
    "    os.mkdir('distance_cor/goals')\n",
    "\"\"\"\n",
    "for continent in continents:\n",
    "    cov_goals_continents_2[continent].to_csv(r'distance_cor/goals/{}_dcov_goals.csv'.format(continent))\n",
    "    cor_goals_continents_2[continent].to_csv(r'distance_cor/goals/{}_dcor_goals.csv'.format(continent))\n",
    "\"\"\"\n",
    "g_cov = open('distance_cor/goals/dcov_goals_continents_2.pkl', 'wb')\n",
    "pickle.dump(cov_goals_continents_2, g_cov)\n",
    "g_cov.close()\n",
    "\n",
    "g_cor = open('distance_cor/goals/dcor_goals_continents_2.pkl', 'wb')\n",
    "pickle.dump(cor_goals_continents_2, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation on goal-level\n",
    "Additionally to the matrices with numbers, we would also like to visualise these matrices and plot these correlations as networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "for continent in continents:\n",
    "    # generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(cor_goals_continents_2[continent].fillna(0), dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(25, 22))\n",
    "\n",
    "    # generate a custom diverging colormap\n",
    "    cmap = sns.color_palette(\"Reds\", 100)\n",
    "\n",
    "    # draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(cor_goals_continents_2[continent].fillna(0), mask=mask, cmap=cmap, vmax=1, center=0.5, vmin=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('{}'.format(continent), fontdict={'fontsize': 52})\n",
    "    plt.savefig('distance_cor/goals/{}_cor_goals.png'.format(continent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation for networkX\n",
    "dcor_dict_g = {}\n",
    "\n",
    "for continent in cor_goals_continents_2.keys():\n",
    "    dcor_dict_g[continent] = {}\n",
    "\n",
    "    for goalcombination in g_combinations:\n",
    "        dcor_dict_g[continent][goalcombination] = cor_goals_continents_2[continent].loc[goalcombination[1], goalcombination[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for continent in cor_goals_continents_2.keys():\n",
    "    for key in dcor_dict_g[continent].keys():\n",
    "        if key[1] == 'T':\n",
    "            dcor_dict_g[continent][tuple((key[0], '18'))] = dcor_dict_g[continent].pop(tuple((key[0], 'T')))\n",
    "        elif key[0] == 'T':\n",
    "            dcor_dict_g[continent][tuple(('18', key[1]))] = dcor_dict_g[continent].pop(tuple(('T', key[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting networks with weighted edges\n",
    "\n",
    "layout = 'circular'\n",
    "\n",
    "centrality_C = {}     # dictionary to save centralities\n",
    "degree_C = {}    # dictionary to save degrees\n",
    "density_C = {}    # dictionary to save weighted densities\n",
    "p_C = {}    # auxiliary\n",
    "partition_C = {}    # dictionary to save clusters\n",
    "\n",
    "for continent in cor_goals_continents_2.keys():\n",
    "    G_C = nx.Graph()\n",
    "\n",
    "    for key, value in dcor_dict_g[continent].items():\n",
    "        G_C.add_edge(int(key[0]), int(key[1]), weight=value, color=sns.color_palette(\"Reds\", 100)[int(np.around(value*100))], alpha=value)\n",
    "        \n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G_C)\n",
    "    elif layout == 'spring':\n",
    "        pos = nx.spring_layout(G_C)\n",
    "    \n",
    "    plt.figure(figsize=(24,16))\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G_C, pos, node_size=1000)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G_C, pos, font_size=46, font_family='sans-serif')\n",
    "\n",
    "    nodes = G_C.nodes()\n",
    "    edges = G_C.edges()\n",
    "    colors = [G_C[u][v]['color'] for u,v in edges]\n",
    "    weights = [G_C[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "    nx.draw_networkx(G_C, pos, with_labels=False, edges=edges, edge_color=colors, node_color='white', node_size=1000, width=np.multiply(weights,20))\n",
    "\n",
    "    ax=plt.gca()\n",
    "    fig=plt.gcf()\n",
    "    trans = ax.transData.transform\n",
    "    trans_axes = fig.transFigure.inverted().transform\n",
    "    imsize = 0.08    # this is the image size\n",
    "    plt.title('{}'.format(continent), fontdict={'fontsize': 52})\n",
    "\n",
    "    for node in G_C.nodes():\n",
    "        (x,y) = pos[node]   \n",
    "        xx,yy = trans((x,y)) # figure coordinates\n",
    "        xa,ya = trans_axes((xx,yy)) # axes coordinates\n",
    "        a = plt.axes([xa-imsize/2.0,ya-imsize/2.0, imsize, imsize])\n",
    "        a.imshow(mpimg.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(node)))\n",
    "        a.axis('off')\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_{}_network_logos.png'.format(continent, layout), format='png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # weighted centrality\n",
    "    centr = nx.eigenvector_centrality(G_C, weight='weight')\n",
    "    centrality_C[continent] = sorted((v, '{:0.2f}'.format(c)) for v, c in centr.items())\n",
    "    \n",
    "    degree_C[continent] = dict(G_C.degree(weight='weight'))\n",
    "    \n",
    "    # weighted density\n",
    "    density_C[continent] = 2 * np.sum(weights) / (len(nodes) * (len(nodes) - 1))\n",
    "    \n",
    "    # weighted clustering with Louvain algorithm\n",
    "    part_C = {}\n",
    "    modularity_C = {}\n",
    "    for i in range(100):\n",
    "        part_C[i] = community.best_partition(G_C, random_state=i)\n",
    "        modularity_C[i] = community.modularity(part_C[i], G_C)\n",
    "    \n",
    "    p_C[continent] = part_C[max(modularity_C, key=modularity_C.get)]\n",
    "\n",
    "    # having lists with nodes being in different clusters\n",
    "    partition_C[continent] = {}\n",
    "    for com in set(p_C[continent].values()) :\n",
    "        partition_C[continent][com] = [nodes for nodes in p_C[continent].keys() if p_C[continent][nodes] == com]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clusters\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(partition_C[continent])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centralities\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(centrality_C[continent])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(degree_C[continent])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# densities\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(density_C[continent])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(goal):\n",
    "    return OffsetImage(plt.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(goal)), zoom=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for continent in cor_goals_continents_2.keys():\n",
    "    # separating goals from their centralities\n",
    "    x = []\n",
    "    y = []\n",
    "    for cent in centrality_C[continent]:\n",
    "        x.append(cent[0])\n",
    "        y.append(float(cent[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18,12))\n",
    "    plt.title('{}'.format(continent), fontdict={'fontsize': 52})\n",
    "    ax.scatter(x, y) \n",
    "    \n",
    "    # adding images\n",
    "    for x0, y0, goal in zip(x, y, list(nodes)):\n",
    "        ab = AnnotationBbox(get_image(goal), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.yaxis.grid()\n",
    "    ax.ylim(0, 0.6)\n",
    "    ax.ylabel('eigenvector centrality')\n",
    "    ax.xlabel('SDGs')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_eigenvector_centrality.png'.format(continent), format='png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting clusters in networks with weighted edges\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "layout = 'spring'\n",
    "\n",
    "for continent in cor_goals_continents_2.keys():\n",
    "    G_C = nx.Graph()\n",
    "\n",
    "    for key, value in dcor_dict_g[continent].items():\n",
    "        G_C.add_edge(int(key[0]), int(key[1]), weight=value, color=sns.color_palette(\"Reds\", 100)[int(np.around(value*100))], alpha=value)\n",
    "        \n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G_C)\n",
    "    elif layout == 'spring':\n",
    "        pos = nx.spring_layout(G_C, iterations=100, seed=42)\n",
    "    \n",
    "    plt.figure(figsize=(24,16))\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G_C, pos, node_size=1000)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G_C, pos, font_size=46, font_family='sans-serif')\n",
    "\n",
    "    nodes = G_C.nodes()\n",
    "    edges = G_C.edges()\n",
    "    colors = [G_C[u][v]['color'] for u,v in edges]\n",
    "    weights = [G_C[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "    nx.draw_networkx(G_C, pos, with_labels=False, edges=edges, edge_color=colors, node_color='white', node_size=1000, width=np.multiply(weights,20))\n",
    "\n",
    "    ax=plt.gca()\n",
    "    fig=plt.gcf()\n",
    "    trans = ax.transData.transform\n",
    "    trans_axes = fig.transFigure.inverted().transform\n",
    "    imsize = 0.08    # this is the image size\n",
    "    plt.title('{}'.format(continent), fontdict={'fontsize': 52})\n",
    "\n",
    "    for node in G_C.nodes():\n",
    "        x,y = pos[node]   \n",
    "        xx,yy = trans((x,y)) # figure coordinates\n",
    "        xa,ya = trans_axes((xx,yy)) # axes coordinates\n",
    "        a = plt.axes([xa-imsize/2.0,ya-imsize/2.0, imsize, imsize])\n",
    "        a.imshow(mpimg.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(node)))\n",
    "        a.axis('off')\n",
    "    \n",
    "    # clusters as patches\n",
    "    patches = []\n",
    "    for com, goals in partition_C[continent].items():\n",
    "        position = []\n",
    "        for goal in goals:\n",
    "            x,y = pos[goal]\n",
    "            position.append((x,y))\n",
    "        \n",
    "        positions = []\n",
    "        for i in range(6000):\n",
    "            np.random.shuffle(position)\n",
    "            positions.extend(position)\n",
    "        \n",
    "        # polygens\n",
    "        polygon = Polygon(positions, closed=False)\n",
    "        patches.append(polygon)\n",
    "    \n",
    "    np.random.seed(72)\n",
    "    colors = 100*np.random.rand(len(patches))\n",
    "    p = PatchCollection(patches, alpha=0.4)\n",
    "    p.set_array(np.array(colors))\n",
    "    ax.add_collection(p)\n",
    "        \n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_{}_network_logos_patches.png'.format(continent, layout), format='png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groups\n",
    "\n",
    "# ALIGN WITH CHANGED CODE FOR CONTINENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "dict_cov_goals_groups_2 = pickle.load(open('distance_cor/goals/dict_cov_goals_groups_2.pkl', 'rb'))\n",
    "dict_cor_goals_groups_2 = pickle.load(open('distance_cor/goals/dict_cor_goals_groups_2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "groups_prep_g = {}\n",
    "groups_prep_boot_g = {}\n",
    "\n",
    "for group in groups:\n",
    "    print(group)\n",
    "    \n",
    "    groups_prep_g[group] = pd.DataFrame(columns=period, index=goals)\n",
    "    groups_prep_boot_g[group] = pd.DataFrame(columns=period, index=goals)\n",
    "    \n",
    "    for goal in goals:\n",
    "        max_year_g = 0\n",
    "        \n",
    "        for year in period:\n",
    "            y_g = []\n",
    "\n",
    "            for country in groups[group].dropna():\n",
    "                y_g.extend(goals_values_i[country].loc[str(goal), year])\n",
    "            \n",
    "            groups_prep_g[group].loc[goal, year] = y_g\n",
    "    \n",
    "            # finding year with most measurements\n",
    "            len_year_g = len(y_g)\n",
    "            if len_year_g > max_year_g:\n",
    "                max_year_g = len_year_g\n",
    "                \n",
    "        # bootstrap to have same number of samples in each year\n",
    "        for year in period:\n",
    "            # not considering the years without samples\n",
    "            if len(groups_prep_g[group].loc[goal, year]) == 0:\n",
    "                continue    \n",
    "            else:\n",
    "                groups_prep_boot_g[group].loc[goal, year] = np.random.choice(groups_prep_g[group].loc[goal, year], size=max_year_g, replace=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups\n",
    "# have conditions in joint random vector\n",
    "dict_cov_goals_groups_2 = {}\n",
    "dict_cor_goals_groups_2 = {}\n",
    "\n",
    "for group in groups:\n",
    "    print(group)\n",
    "    \n",
    "    dict_cov_goa_g = pairs_g0.copy()\n",
    "    dict_cor_goa_g = pairs_g0.copy()\n",
    "    \n",
    "    for i, row in pairs_g0.iterrows():\n",
    "        #print(row)\n",
    "        \n",
    "        if row['condition'] == 0:            \n",
    "            # pairwise distance correlation\n",
    "            dict_cov_goa_g.loc[i, 'dcov'] = dcor.distance_covariance(np.array(groups_prep_boot_g[group].loc[row['pair_0']].to_list()), np.array(groups_prep_boot_g[group].loc[row['pair_1']].to_list()))**2\n",
    "            dict_cor_goa_g.loc[i, 'dcor'] = dcor.distance_correlation(np.array(groups_prep_boot_g[group].loc[row['pair_0']].to_list()), np.array(groups_prep_boot_g[group].loc[row['pair_1']].to_list()))**2\n",
    "        \n",
    "        else:\n",
    "            # partial distance correlation\n",
    "            condition = pd.DataFrame(index=period, columns=row['condition'] + ['combined'])\n",
    "            for y in period:\n",
    "                condition.loc[y, 'combined'] = []\n",
    "            for c in row['condition']:\n",
    "                condition[c] = groups_prep_boot_g[group].loc[c]\n",
    "                for y in period:\n",
    "                    condition.loc[y, 'combined'].extend(condition.loc[y, c])\n",
    "                    \n",
    "            # square partial distance correlation to range [0, 1]    \n",
    "            dict_cov_goa_g.loc[i, 'dcov'] = dcor.partial_distance_covariance(np.array(groups_prep_boot_g[group].loc[row['pair_0']].to_list()), np.array(groups_prep_boot_g[group].loc[row['pair_1']].to_list()), np.array(condition['combined'].to_list()))**2\n",
    "            dict_cor_goa_g.loc[i, 'dcor'] = dcor.partial_distance_correlation(np.array(groups_prep_boot_g[group].loc[row['pair_0']].to_list()), np.array(groups_prep_boot_g[group].loc[row['pair_1']].to_list()), np.array(condition['combined'].to_list()))**2\n",
    "    \n",
    "    # find minimum distance correlation between any two goals\n",
    "    dict_cov_goa_gr = dict_cov_goa_g.groupby(['pair_0', 'pair_1'])['dcov'].apply(list).reset_index(name='list_dcov')\n",
    "    dict_cor_goa_gr = dict_cor_goa_g.groupby(['pair_0', 'pair_1'])['dcor'].apply(list).reset_index(name='list_dcor')\n",
    "    \n",
    "    for i, row_g in dict_cov_goa_gr.iterrows():\n",
    "        dict_cov_goa_gr.loc[i, 'min_dcov'] = min(dict_cov_goa_gr.loc[i, 'list_dcov'])\n",
    "        dict_cor_goa_gr.loc[i, 'min_dcor'] = min(dict_cor_goa_gr.loc[i, 'list_dcor'])\n",
    "       \n",
    "    dict_cov_goals_groups_2[group] = dict_cov_goa_gr\n",
    "    dict_cor_goals_groups_2[group] = dict_cor_goa_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "g_cov = open('distance_cor/goals/dict_cov_goals_groups_2.pkl', 'wb')\n",
    "pickle.dump(dict_cov_goals_groups_2, g_cov)\n",
    "g_cov.close()\n",
    "\n",
    "g_cor = open('distance_cor/goals/dict_cor_goals_groups_2.pkl', 'wb')\n",
    "pickle.dump(dict_cor_goals_groups_2, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    for i, row in dict_cov_goals_groups_2[group].iterrows():\n",
    "        dict_cov_goals_groups_2[group].loc[i, 'max_dcov'] = max(dict_cov_goals_groups_2[group].loc[i, 'list_dcov'])\n",
    "        dict_cor_goals_groups_2[group].loc[i, 'max_dcor'] = max(dict_cor_goals_groups_2[group].loc[i, 'list_dcor'])\n",
    "        dict_cov_goals_groups_2[group].loc[i, 'avg_dcov'] = np.mean(dict_cov_goals_groups_2[group].loc[i, 'list_dcov'])\n",
    "        dict_cor_goals_groups_2[group].loc[i, 'avg_dcor'] = np.mean(dict_cor_goals_groups_2[group].loc[i, 'list_dcor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_goals_groups_2 = {}\n",
    "cor_goals_groups_2 = {}\n",
    "\n",
    "for group in groups:\n",
    "    cov_goals_groups_2[group] = pd.DataFrame(index=goals, columns=goals)\n",
    "    cor_goals_groups_2[group] = pd.DataFrame(index=goals, columns=goals)\n",
    "\n",
    "    for i in list(dict_cov_goals_groups_2[group].index):\n",
    "        goal_0 = dict_cov_goals_groups_2[group].loc[i, 'pair_0']\n",
    "        goal_1 = dict_cov_goals_groups_2[group].loc[i, 'pair_1']\n",
    "        \n",
    "        cov_goals_groups_2[group].loc[goal_1, goal_0] = np.sqrt(dict_cov_goals_groups_2[group].loc[i, 'min_dcov'])\n",
    "        cor_goals_groups_2[group].loc[goal_1, goal_0] = np.sqrt(dict_cor_goals_groups_2[group].loc[i, 'min_dcor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "if not os.path.exists('distance_cor/goals'):\n",
    "    os.mkdir('distance_cor/goals')\n",
    "\"\"\"\n",
    "for group in groups:\n",
    "    cov_goals_groups_2[group].to_csv(r'distance_cor/goals/{}_dcov_goals.csv'.format(group))\n",
    "    cor_goals_groups_2[group].to_csv(r'distance_cor/goals/{}_dcor_goals.csv'.format(group))\n",
    "\"\"\"    \n",
    "g_cov = open('distance_cor/goals/dcov_goals_groups_2.pkl', 'wb')\n",
    "pickle.dump(cov_goals_groups_2, g_cov)\n",
    "g_cov.close()\n",
    "\n",
    "g_cor = open('distance_cor/goals/dcor_goals_groups_2.pkl', 'wb')\n",
    "pickle.dump(cor_goals_groups_2, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation on goal-level\n",
    "Additionally to the matrices with numbers, we would also like to visualise these matrices and plot these correlations as networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups\n",
    "for group in groups:\n",
    "    # generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(cor_goals_groups_2[group].fillna(0), dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(25, 22))\n",
    "\n",
    "    # generate a custom diverging colormap\n",
    "    cmap = sns.color_palette(\"Reds\", 100)\n",
    "\n",
    "    # draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(cor_goals_groups_2[group].fillna(0), mask=mask, cmap=cmap, vmax=1, center=0.5, vmin=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('{}'.format(group), fontdict={'fontsize': 52})\n",
    "    plt.savefig('distance_cor/goals/{}_cor_goals.png'.format(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation for networkX\n",
    "dcor_dict_g = {}\n",
    "\n",
    "for group in cor_goals_groups_2.keys():\n",
    "    dcor_dict_g[group] = {}\n",
    "\n",
    "    for goalcombination in g_combinations:\n",
    "        dcor_dict_g[group][goalcombination] = cor_goals_groups_2[group].loc[goalcombination[1], goalcombination[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in cor_goals_groups_2.keys():\n",
    "    for key in dcor_dict_g[group].keys():\n",
    "        if key[1] == 'T':\n",
    "            dcor_dict_g[group][tuple((key[0], '18'))] = dcor_dict_g[group].pop(tuple((key[0], 'T')))\n",
    "        elif key[0] == 'T':\n",
    "            dcor_dict_g[group][tuple(('18', key[1]))] = dcor_dict_g[group].pop(tuple(('T', key[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting networks with weighted edges\n",
    "\n",
    "layout = 'circular'\n",
    "\n",
    "centrality_G = {}    # dictionary to save centralities\n",
    "degree_G = {}     # dictionary to save degrees\n",
    "density_G = {}    # dictionary to save weighted densities\n",
    "p_G = {}    # auxiliary\n",
    "partition_G = {}    # dictionary to save clusters\n",
    "\n",
    "for group in cor_goals_groups_2.keys():\n",
    "    G_G = nx.Graph()\n",
    "\n",
    "    for key, value in dcor_dict_g[group].items():\n",
    "        G_G.add_edge(int(key[0]), int(key[1]), weight=value, color=sns.color_palette(\"Reds\", 100)[int(np.around(100*value))], alpha=value)\n",
    "        \n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G_G)\n",
    "    elif layout == 'spring':\n",
    "        pos = nx.spring_layout(G_G)\n",
    "    \n",
    "    plt.figure(figsize=(24,16))\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G_G, pos, node_size=1000)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G_G, pos, font_size=46, font_family='sans-serif')\n",
    "\n",
    "    nodes = G_G.nodes()\n",
    "    edges = G_G.edges()\n",
    "    colors = [G_G[u][v]['color'] for u,v in edges]\n",
    "    weights = [G_G[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "    nx.draw_networkx(G_G, pos, with_labels=False, edges=edges, edge_color=colors, node_color='white', node_size=1000, width=np.multiply(weights,20))\n",
    "\n",
    "    ax=plt.gca()\n",
    "    fig=plt.gcf()\n",
    "    trans = ax.transData.transform\n",
    "    trans_axes = fig.transFigure.inverted().transform\n",
    "    imsize = 0.08    # this is the image size\n",
    "    plt.title('{}'.format(group), fontdict={'fontsize': 52})\n",
    "\n",
    "    for node in G_G.nodes():\n",
    "        (x,y) = pos[node]   \n",
    "        xx,yy = trans((x,y)) # figure coordinates\n",
    "        xa,ya = trans_axes((xx,yy)) # axes coordinates\n",
    "        a = plt.axes([xa-imsize/2.0,ya-imsize/2.0, imsize, imsize])\n",
    "        a.imshow(mpimg.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(node)))\n",
    "        a.axis('off')\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_{}_network_logos.png'.format(group, layout), format='png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # centrality\n",
    "    centr = nx.eigenvector_centrality(G_G, weight='weight')\n",
    "    centrality_G[group] = sorted((v, '{:0.2f}'.format(c)) for v, c in centr.items())\n",
    "    \n",
    "    degree_G[group] = dict(G_G.degree(weight='weight'))\n",
    "    \n",
    "    # weighted density\n",
    "    density_G[group] = 2 * np.sum(weights) / (len(nodes) * (len(nodes) - 1))\n",
    "    \n",
    "    # weighted clustering with Louvain algorithm\n",
    "    p_G[group] = community.best_partition(G_G)\n",
    "\n",
    "    # having lists with nodes being in different clusters\n",
    "    partition_G[group] = {}\n",
    "    for com in set(p_G[group].values()) :\n",
    "        partition_G[group][com] = [nodes for nodes in p_G[group].keys() if p_G[group][nodes] == com]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clusters\n",
    "for group in groups:\n",
    "    print(group)\n",
    "    print(partition_G[group])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    print(group)\n",
    "    print(centrality_G[group])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    print(group)\n",
    "    print(degree_G[group])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    print(group)\n",
    "    print(density_G[group])\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in cor_goals_groups_2.keys():\n",
    "    # separating goals from their centralities\n",
    "    x = []\n",
    "    y = []\n",
    "    for cent in centrality_G[group]:\n",
    "        x.append(cent[0])\n",
    "        y.append(float(cent[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18,12))\n",
    "    plt.title('{}'.format(group), fontdict={'fontsize': 52})\n",
    "    ax.scatter(x, y) \n",
    "    \n",
    "    # adding images\n",
    "    for x0, y0, goal in zip(x, y, list(nodes)):\n",
    "        ab = AnnotationBbox(get_image(goal), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.yaxis.grid()\n",
    "    ax.ylim(0, 0.6)\n",
    "    ax.ylabel('eigenvector centrality')\n",
    "    ax.xlabel('SDGs')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_eigenvector_centrality.png'.format(group), format='png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting clusters in networks with weighted edges\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "layout = 'spring'\n",
    "\n",
    "for group in cor_goals_groups_2.keys():\n",
    "    G_G = nx.Graph()\n",
    "\n",
    "    for key, value in dcor_dict_g[group].items():\n",
    "        G_G.add_edge(int(key[0]), int(key[1]), weight=value, color=sns.color_palette(\"Reds\", 100)[int(np.around(100*value))], alpha=value)\n",
    "        \n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G_G)\n",
    "    elif layout == 'spring':\n",
    "        pos = nx.spring_layout(G_G, iterations=100, seed=42)\n",
    "    \n",
    "    plt.figure(figsize=(24,16))\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G_G, pos, node_size=1000)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G_G, pos, font_size=46, font_family='sans-serif')\n",
    "\n",
    "    nodes = G_G.nodes()\n",
    "    edges = G_G.edges()\n",
    "    colors = [G_G[u][v]['color'] for u,v in edges]\n",
    "    weights = [G_G[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "    nx.draw_networkx(G_G, pos, with_labels=False, edges=edges, edge_color=colors, node_color='white', node_size=1000, width=np.multiply(weights,20))\n",
    "\n",
    "    ax=plt.gca()\n",
    "    fig=plt.gcf()\n",
    "    trans = ax.transData.transform\n",
    "    trans_axes = fig.transFigure.inverted().transform\n",
    "    imsize = 0.08    # this is the image size\n",
    "    plt.title('{}'.format(group), fontdict={'fontsize': 52})\n",
    "\n",
    "    for node in G_G.nodes():\n",
    "        (x,y) = pos[node]   \n",
    "        xx,yy = trans((x,y)) # figure coordinates\n",
    "        xa,ya = trans_axes((xx,yy)) # axes coordinates\n",
    "        a = plt.axes([xa-imsize/2.0,ya-imsize/2.0, imsize, imsize])\n",
    "        a.imshow(mpimg.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(node)))\n",
    "        a.axis('off')\n",
    "    \n",
    "    # clusters as patches\n",
    "    patches = []\n",
    "    for com, goals in partition_G[group].items():\n",
    "        position = []\n",
    "        for goal in goals:\n",
    "            x,y = pos[goal]\n",
    "            position.append((x,y))\n",
    "        \n",
    "        positions = []\n",
    "        for i in range(6000):\n",
    "            np.random.shuffle(position)\n",
    "            positions.extend(position)\n",
    "        \n",
    "        # polygens\n",
    "        polygon = Polygon(positions, closed=False)\n",
    "        patches.append(polygon)\n",
    "    \n",
    "    np.random.seed(72)\n",
    "    colors = 100*np.random.rand(len(patches))\n",
    "    p = PatchCollection(patches, alpha=0.4)\n",
    "    p.set_array(np.array(colors))\n",
    "    ax.add_collection(p)\n",
    "\n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_{}_network_logos_patches.png'.format(group, layout), format='png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
