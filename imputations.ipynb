{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing values\n",
    "\n",
    "We work with the data set how it is present now and apply two disparate methods, each computing the imputations for missing values differently. \n",
    "\n",
    "1) The first method is a **weighted k nearest neighbour (w-kNN)** algorithm, which imputes missing values with weights equal to the inverse Euclidean distance. \n",
    "\n",
    "**Assumptions:** we believe the values missing lie in between the boundaries of the highest and lowest value present in the data set. This might work well for most, but it could also be that the missing values are outliers.\n",
    "\n",
    "2) The second method is **Gaussian process (GP) regression** where we take the mean function as the imputation for missing values.\n",
    "\n",
    "**Assumptions:** we believe underlying time-series are continuous, i.e. we can have smooth approximations of the available time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupings` and `countries`\n",
    "\n",
    "We want to **remind** ourselves that our list `groupings` does not only contain countries, it also contains groups of countries like continents, economic zones, etc. and islands which belong to certain countries, but could somehow be very different as e.g. territories. We save all these entries in a new list `countries`. \n",
    "\n",
    "Furthermore, some sub-indicators are of ordinal type, i.e. they are defined as 'number of countries which...'. For all countries, we have here either a 1 or a 0 for 'yes' or 'no', respectively. For all other groupings, we have there most likely a number larger than 1. This could be tricky for our imputations later, because these are based on the similarity of two groupings and these similarities could be strong between, say, France and the World Trade Organisation (WTO). If the WTO had a missing value for an ordinal sub-indicator, the imputation would most likely be very similar to the one of France, so 1 or 0. But this is unrealistic, and all other countries being member of the WTO and behave almost as similar as France, would make the imputation just closer to 1 and not larger than 1.\n",
    "\n",
    "The solution is simple, but time-consuming: we only impute missing values for `countries`. Then, we compute the averages and sums for continuous and ordinal sub-indicators, respectively, of the countries which form these non-country groupings. \n",
    "\n",
    "It's time-consuming because we need to do it by hand. I prefer doing it in a GUI and export the csv file to be able to quickly delete non-countries from the list `groupings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT (we don't want to re-run the entire script every time we continue working on it)\n",
    "dict_all_s = pickle.load(open('dict_all_s.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print('Standardised values: ')\n",
    "print(dict_all_s['Africa']['2000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we open the `csv` file in a GUI and delete the groupings which are *not* countries or part of countries. We call these non-country groupings and examples are North America, Western Asia, Least Developed Countries (LDC), Land Locked Developing Countries (LLDC), Small Island Developing States (SIDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read amended csv file\n",
    "c = pd.read_csv('utils/countries.csv', dtype=str)\n",
    "countries = list(c['Countries'])\n",
    "\n",
    "#check\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1) weighted k nearest neighbour (w-kNN)\n",
    "\n",
    "The w-kNN algorithm is straightforward: we calculate how similar countries are in each given year $y$ with the standardised Euclidean distance $E_y$ and take the inverse of the absolute $|E_y|$ as the weight to impute missing values in any given country $c_i$ in each given year $y$ for each given sub-indicator $j$.\n",
    "\n",
    "A good start to understand this algorithm is to understand high-dimensional space: https://youtu.be/wvsE8jm1GzE\n",
    "\n",
    "### Euclidean distance\n",
    "The Euclidean distance $e_y$ for year $y$ for any given pair of countries $(c_i, c_k)$ for any given sub-indicator $j$ is calculated by:\n",
    "$$ e_y(c_{i}, c_{k}) = \\lVert c_{i}, c_{k} \\rVert_2 = \\sqrt{(c_{ij} - c_{kj})^2} $$\n",
    "\n",
    "We calculate the squared distances between any given pair of countries $(c_i, c_k)$, but do not consider the country $k+1$ which has the largest distance $e_y$ to country $i$. We do so for any given sub-indicator $j$ and take the square root of it. $c_{ij}$ is the sub-indicator $j$ of country $i$, and $i \\neq k$. Thus, any unique pair of countries $i$ an and $k$, $i \\neq k$, has **one** Euclidean distance $e_y$ for year $y$ only.\n",
    "\n",
    "Afterwards, we standardise this with respect to the country $k+1$ which has the largest distance $e_y$ to country $i$ by the following equation:\n",
    "$$ E_y(c_{i}, c_{k}) = \\frac{e_y(c_{i}, c_{k})}{e_y(c_{i}, c_{k+1})} $$\n",
    "\n",
    "\n",
    "### Imputations\n",
    "We want that our imputations $x^{j'}_{i,y}$ for missing sub-indicator $j'$ in country $i$ in year $y$ are similar to sub-indicators $j$ of countries $k$ which have a **small** Euclidean distance $E_y$ and dissimilar to sub-indicators $j$ of countries $k$ which have a **large** Euclidean distance $E_y$. Consequently, the imputations $x^{j'}_{i,y}$ are the *weighted* averages where the *weights* are equal to the inverse standardised Euclidean distance $\\frac{1}{|E_y(c_{i}, c_{k})|}$.\n",
    "\n",
    "First, we compute $E_y$ for all available pairs of sub-indicators $j$ amongst two countries $i$ and $k$. Since countries have different amounts of available data points, we average by multiplying the sum by $1/J$, where $J$ is the total number of sub-indicators taken into account here. Note, this does not necessarily be 375, because we have missing values for many sub-indicators. Second, we sum over $k$ to add together all weighted $x^j_{k,y}$ of each unique pair of countries $i$ and $k$ and compute its average by dividing by $K$.\n",
    "\n",
    "$$ x^{j'}_{i,y} = \\frac{1}{K} \\sum_k \\frac{1}{|E_y(c_{i}, c_{k})|} \\cdot x^j_{k,y} $$\n",
    "\n",
    "**Assumptions:** we calculate how similar countries are according to their values for *all* sub-indicators in a given year. We assume that the specific sub-indicators which do not have values in this given year are exactly as similar as the ones which we can calculate a distance for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Let's have a final check before we start our w-kNN algorithm to compute all missing values. We have here negative values which might sound confusing, but bear in mind that we have standardised the data before, i.e. the data distribution has mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "dict_all_s['Iraq (Central Iraq)'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "dict_e = pickle.load(open('utils/distances_unstand.pkl', 'rb'))\n",
    "dict_E = pickle.load(open('utils/distances_stand.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "print('Euclidean distance: ', dict_E[('Azerbaijan', 'Angola', '2000')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Calculating the Euclidean distance\n",
    "\n",
    "We can calculate the standardised distance $E_y(c_{i}, c_{k})$ after having prepared everything. We do this for each unique pair of two *countries* in each year. In other words, we do not want to calculate $E_y(c_{i}, c_{k})$ for $i = k$ and $E_y(c_{i}, c_{k}) = E_y(c_{k}, c_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python package <code>itertools</code> can help us generating the unique pairs of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# create list out of all unique combinations\n",
    "countrycombinations = list(itertools.combinations_with_replacement(countries, 2))\n",
    "countrycombinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# check\n",
    "countrycombinations[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Here, we calculate the standardised distance $E_y(c_{i}, c_{k})$ for each unique pair of two countries in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "First, we compute the (not standardised) distances $e_y$ and insert them into a new dictionary `dict_e`.\n",
    "\n",
    "While exploring the data, we see that nearly no data are available for the years `1990` to `1999`. Consequently, imputations in those years will be based on very weak foundations and we do not consider these years for now. For our similarity investigations later, it does not matter much how many data points we have totally available per country, it is more important that all countries have the same amount of data points. For now, we also omit data for the year `2019`, because it seems not all countries have reported their data yet. Hence, there aren't too many data points available neither.\n",
    "\n",
    "We set the `period` of years we want to consider in our computations for $e_y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "period = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call seriescodes again\n",
    "info = pd.read_csv('utils/info.csv', dtype=str)\n",
    "seriescodes = list(info['SeriesCode'])\n",
    "seriescodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very memory-intensive (~3 hours computing time)\n",
    "# no need to run every time again, just see CHECKPOINT above and load pickle file\n",
    "\n",
    "dict_e = {}    \n",
    "\n",
    "for year in period:\n",
    "    \n",
    "    for countrycombination in countrycombinations:\n",
    "        \n",
    "        country0_e = []    # create two empty lists for the two groupings we consider at the moment\n",
    "        country1_e = []    # these lists contain series codes with data available in both groupings\n",
    "        j = 0    # counter\n",
    "        \n",
    "        for seriescode in seriescodes:\n",
    "            # we can only consider sub-indicators with data available in both groupings\n",
    "            if pd.isna(dict_all_s[countrycombination[0]][year][seriescode]) is False and pd.isna(dict_all_s[countrycombination[1]][year][seriescode]) is False:\n",
    "                country0_e.append(dict_all_s[countrycombination[0]][year][seriescode])\n",
    "                country1_e.append(dict_all_s[countrycombination[1]][year][seriescode])\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        print('number of data points available: ', j)    # check\n",
    "        if j > 0:\n",
    "            e = euclidean(country0_e, country1_e)\n",
    "        else:\n",
    "            e = np.nan    # make NaN, so we can later assign 1/E = 0 easily\n",
    "            j = 1\n",
    "            \n",
    "        print('e in {} between {} and {}:'.format(year, countrycombination[0], countrycombination[1]), e/j)\n",
    "        \n",
    "        dict_e[year, countrycombination[1], countrycombination[0]] = e/j\n",
    "        dict_e[year, countrycombination[0], countrycombination[1]] = dict_e[year, countrycombination[1], countrycombination[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better save this precious data\n",
    "f = open('utils/distances_unstand.pkl', 'wb')\n",
    "pickle.dump(dict_E, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dict_e['2000', 'British Virgin Islands', 'Martinique']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise these distances $e_y$ and save them in `dict_E`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_E = copy.deepcopy(dict_e)\n",
    "\n",
    "for year in period:\n",
    "    \n",
    "    list_e = []    # list with all distances in each year\n",
    "    max_e = 0    # maximum value in list_e\n",
    "    dict_e_year = {}    # auxiliary dictionary per year\n",
    "    \n",
    "    \n",
    "    for k in dict_e.keys():\n",
    "        if year in k:\n",
    "            dict_e_year[k] = dict_e[k]\n",
    "            \n",
    "\n",
    "    max_e = np.nanmax(list(dict_e_year.values()))\n",
    "    print('------------')\n",
    "    print('max_e in {}'.format(year), max_e)\n",
    "    print('------------')\n",
    "    list(dict_e_year.values()).remove(max_e)\n",
    "    \n",
    "    \n",
    "    for k in dict_e_year.keys():\n",
    "        print(k)\n",
    "        if np.isnan(dict_e_year[k]) == False:\n",
    "            print('unstandardised distance e:', dict_e_year[k])\n",
    "            dict_E[k] = dict_e_year[k] / max_e    # standardise distance\n",
    "            \n",
    "        else:\n",
    "            dict_E[k] = 999999999999\n",
    "        \n",
    "        print('standardised distance E:', dict_E[k])\n",
    "        print('------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print('unstandardised distance e:', dict_e['2000', 'Afghanistan', 'Colombia'])\n",
    "print('standardised distance E:  ', dict_E['2000', 'Afghanistan', 'Colombia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# better save this precious data\n",
    "f = open('utils/distances_stand.pkl', 'wb')\n",
    "pickle.dump(dict_E, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Imputations for countries\n",
    "\n",
    "Now, we impute the missing values according to the equation we previously derived:\n",
    "\n",
    "$$ x^{j'}_{i,y} = \\frac{1}{K} \\sum_k \\frac{1}{|E_y(c_{i}, c_{k})|} \\cdot x^j_{k,y} $$\n",
    "\n",
    "To recap, our imputations $x^{j'}_{i,y}$ for missing sub-indicator $j'$ in country $i$ in year $y$ should be similar to sub-indicators $j$ of country $k$, according to the inverse standardised Euclidean distance $\\frac{1}{|E_y(c_{i}, c_{k})|}$ between $i$ and $k$. \n",
    "\n",
    "As aforementioned and shown in the equation of $E_y$, $E_y$ is dependent on the number of pairs we have in both countries data for. Our `dict_E` has already entries for $E$ normalised according to this number of available pairs of data. We multiply our weight for each imputation, i.e. the inverse standardised Euclidean distance $\\frac{1}{|E_y(c_{i}, c_{k})|}$ between $i$ and $k$, by the value $x^j_{k,y}$ of the other country $k$ in year $y$ for sub-indicator $j$. We sum over $k$ to add together all weighted $x^j_{k,y}$ of each unique pair of countries $i$ and $k$ and compute its average by dividing by $K$. $K$ is the number of countries which have values available for the sub-indicator $x^{j'}_{k,y}$ to be computed. \n",
    "\n",
    "We also know that some indicators are binary, i.e. 1 for 'yes' and 0 for 'no', and ordinal for groupings of countries. These indicators start usually with 'number of countries which...'. The imputations for these must be handled differently: we round the imputed value to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "dict_all_i = pickle.load(open('utils/dict_all_i.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = ['1.5.3','12.1.1','15.6.1','17.16.1','17.18.2','17.18.3','17.19.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series codes for ordinal indicators\n",
    "ordinal_ser = ['SG_DSR_LEGREG','SG_DSR_LGRGSR','SG_SCP_CNTRY','SG_SCP_CORMEC','SG_SCP_MACPOL','SG_SCP_POLINS','ER_CBD_ABSCLRHS','ER_CBD_NAGOYA','ER_CBD_ORSPGRFA','ER_CBD_PTYPGRFA','ER_CBD_SMTA','SG_PLN_MSTKSDG','SG_STT_FPOS','SG_STT_NSDSFDDNR','SG_STT_NSDSFDGVT','SG_STT_NSDSFDOTHR','SG_STT_NSDSFND','SG_STT_NSDSIMPL','SG_REG_BRTH90','SG_REG_BRTH90N','SG_REG_CENSUS','SG_REG_CENSUSN','SG_REG_DETH75','SG_REG_DETH75N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# memory-intensive (~3 hours)\n",
    "\n",
    "dict_all_i = copy.deepcopy(dict_all_s)\n",
    "\n",
    "for country in countries: \n",
    "    \n",
    "    not_countries = [c for c in countries if c != country]\n",
    "    \n",
    "    for seriescode in seriescodes:\n",
    "        for year in period:            \n",
    "            if pd.isna(dict_all_s[country][year][seriescode]) is True:\n",
    "                K = 0\n",
    "                all_k = []\n",
    "                \n",
    "                for not_country in not_countries: \n",
    "                    if pd.isna(dict_all_s[not_country][year][seriescode]) is False:    # not_country can also be NaN -> exclude those\n",
    "                        K += 1\n",
    "                        k = 1/(dict_E[(year, country, not_country)]) * dict_all_s[not_country][year][seriescode]\n",
    "                        all_k.append(k)\n",
    "                        sum_k = np.sum(all_k)\n",
    "                    \n",
    "                print('K =', K)\n",
    "                    \n",
    "                if K > 0:\n",
    "                    print('sum k =', sum_k)\n",
    "                                                                \n",
    "                    if seriescode in ordinal_ser:\n",
    "                        dict_all_i[country][year][seriescode] = np.around(sum_k / K)    # round to have integer\n",
    "                    else:\n",
    "                        dict_all_i[country][year][seriescode] = sum_k / K\n",
    "                            \n",
    "                else:\n",
    "                    dict_all_i[country][year][seriescode] = np.nan\n",
    "                    \n",
    "\n",
    "                \n",
    "                print('Imputation for {} in {} in {}'.format(seriescode, country, year), dict_all_i[country][year][seriescode])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "print('NaN here', dict_all_s['Iraq (Central Iraq)']['2009']['SI_POV_DAY1'])\n",
    "print('Imputed value', dict_all_i['Iraq (Central Iraq)']['2009']['SI_POV_DAY1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputations for non-country groupings\n",
    "\n",
    "These were the imputations for all countries, but we would like to have imputations for non-country groupings, too. As aforementioned, we compute the averages and sums for continuous and ordinal sub-indicators, respectively, of the countries which form these non-country groupings. \n",
    "\n",
    "Unfortunately, we need to define these non-country groupings by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the sums and averages and have our imputations complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_countries = [g for g in groupings if g != countries]\n",
    "\n",
    "for non_country in non_countries:\n",
    "    \n",
    "    for seriescode in seriescodes:\n",
    "        for year in period:\n",
    "            if pd.isna(dict_all_s[non_country][year][seriescode]) is True:\n",
    "                    \n",
    "                if seriescode in ordinal_ser:\n",
    "                    dict_all_i[non_country][year][seriescode] = np.sum()\n",
    "                else:\n",
    "                    dict_all_i[non_country][year][seriescode] = np.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save the imputations to have another checkpoint here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# as csv files\n",
    "if not os.path.exists('csv_imputed_s'):\n",
    "    os.mkdir('csv_imputed_s')\n",
    "\n",
    "for group in groupings:\n",
    "    dict_all_i[group].to_csv(r'csv_imputed_s/{}.csv'.format(group))\n",
    "    \n",
    "# as pkl files\n",
    "imp = open('utils/dict_all_i.pkl', 'wb')\n",
    "pickle.dump(dict_all_i, imp)\n",
    "imp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gaussian process regression\n",
    "\n",
    "This method takes less assumptions into account and the assumptions it takes are much more realistic. Basically, we only assume that the data we have form continuous time-series. Given our socio-economic and environmental data, this is highly probable since nothing can drastically change over night in this world.\n",
    "\n",
    "Gaussian processes (GPs) are useful for probabilistic modeling of unknown functions. Each of our underlying time-series which have missing values is an unknown function. GPs generalise the multivariate Gaussian distribution by treating any finite subset of function values as a multivariate, i.e. joint, Gaussian distributions. Any GP is fully specified by a mean vector $\\mathbf{m}$ and covariance matrix $\\mathbf{K}$, whose elements are computed by a mean function $m = m(\\mathbf{x})$ and a covariance function $\\mathbf{K}_{ij} = k(x_i, x_j)$, respectively, where $i$ and $j$ are the years we have measurements available in each time-series. We treat these measurements as random variables in GPs.\n",
    "\n",
    "Conditioning the probability distributions for our imputations on this joint distribution of function values $f$ yields the Gaussian predictive distribution of function values $f'$ at times of missing values. \n",
    "\n",
    "$$p(f' | f) = \\mathcal{N}(\\mu, \\mathbf{\\Sigma})$$\n",
    "$$\\mu = \\mathbf{m}_{f'} + \\mathbf{K}_{f'f} \\mathbf{K}_{ff}^{-1}(\\mathbf{f}-\\mathbf{m}_f)$$\n",
    "$$\\mathbf{\\Sigma} = \\mathbf{K}_{f'f'} - \\mathbf{K}_{f'f} \\mathbf{K}_{ff}^{-1} \\mathbf{K}_{ff'}$$\n",
    "\n",
    "\n",
    "*Some parts from hereon are slightly adapadted from a notebook written by Wil Ward, adapted from notebooks by [Rich Wilkinson](https://rich-d-wilkinson.github.io/) and [Neil Lawrence](http://inverseprobability.com/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see first of all one exemplary time-series, `dict_all_i['Algeria'][period].loc['SI_POV_DAY1']`, to get a basic idea what we could expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making integeres out of floats \n",
    "per = []\n",
    "for p in period:\n",
    "    per.append(int(p))\n",
    "# per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years from 2000 to 2018\n",
    "x = np.array(per).reshape((-1, 1))\n",
    "\n",
    "# standardised values for these X years\n",
    "y = np.array(dict_all_i['Algeria'][period].loc['SI_POV_DAY1']).reshape((-1, 1))\n",
    "\n",
    "# Setup our figure environment\n",
    "plt.figure(figsize=(19, 7))\n",
    "\n",
    "# Plot observations\n",
    "plt.plot(x, y, \"kx\", mew=2)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"years\", size=20), plt.ylabel(\"standardised values\", size=20)\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.legend(labels=[\"measurements\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with defining a covariance function, from hereon referred to as a **kernel**. The most commonly used kernel in machine learning is the Gaussian-form radial basis function (RBF) kernel. \n",
    "\n",
    "The definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as:\n",
    "\n",
    "$$\n",
    "    \\kappa_\\mathrm{rbf}(x_i,x_j) = \\sigma^2\\exp\\left(-\\frac{(x_i-x_j)^2}{2\\mathscr{l}^2}\\right)\n",
    "$$\n",
    "\n",
    "It has two parameters, described as the variance, $\\sigma^2$ and the lengthscale $\\mathscr{l}$.\n",
    "\n",
    "We define our kernels using the input dimension as the first argument, in the simplest case `input_dim=1` for 1-dimensional regression, how it is in our case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1-D RBF kernel with default parameters\n",
    "k = GPy.kern.RBF(1)\n",
    "# Preview the kernel's parameters\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the kernel\n",
    "\n",
    "We can visualise our kernel in a few different ways. One possibility is to plot the *shape* of the kernel at one specific location along the $x$-axis, say $0$, by plotting $k(x,0)$ over some sample space $x$ which, looking at the equation above, clearly has a Gaussian shape. This describes the **covariance between each sample location and $0$** and we can see that it decreases the further we move away from $0$.\n",
    "\n",
    "Alternatively, we can construct a full covariance matrix, $\\mathbf{K}_{xx} := k(x_i,x_j)$ with samples $x_i = x_j$. The resulting GP prior is a multivariate normal distribution over the space of samples $x$: $\\mathcal{N}(\\mathbf{0}, \\mathbf{K}_{xx})$. It should be evident then that the elements of the matrix represent the covariance between respective points in $x_i$ and $x_j$, and that it is exactly $\\sigma^2 = 1$ in the diagonal.\n",
    "\n",
    "We can show this using `pyplot` to plot the vector $k(x_i,0)$ and the matrix $k(x_i,x_j)$ using `k.K(`$\\cdot$ `,` $\\cdot$`)`.\n",
    "\n",
    "We do this first for a too-perfect-for-reality toy data set and define it as $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our toy data set and sample space: 250 samples in the interval [-4,4]\n",
    "X = np.linspace(-4.,4.,250)[:, None] # we need [:, None] to reshape X into a column vector for use in GPy\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "# ==== k(x,0)\n",
    "\n",
    "plt.subplot(121) # left plot\n",
    "\n",
    "# First, sample kernel at x_j = 0\n",
    "k.lengthscale = 1\n",
    "K = k.K(X, np.array([[0.]])) # k(x,0)\n",
    "\n",
    "# Plot covariance vector\n",
    "plt.plot(X,K)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_i$\", size=20), plt.ylabel(\"$\\kappa$\", size=20)\n",
    "plt.title(\"$\\kappa_{rbf}(x,0)$\", size=20)\n",
    "\n",
    "# ==== k(x_i,x_j)\n",
    "\n",
    "plt.subplot(122) # right plot\n",
    "\n",
    "# The kernel takes two inputs, and outputs the covariance between each respective point in the two inputs\n",
    "K = k.K(X,X)\n",
    "\n",
    "# Plot the covariance of the sample space\n",
    "plt.pcolor(X.T, X, K)\n",
    "\n",
    "# Format and annotate plot\n",
    "plt.gca().invert_yaxis(), plt.gca().axis(\"image\")\n",
    "plt.xlabel(\"$x_i$\", size=20), plt.ylabel(\"$x_j$\", size=20), plt.colorbar()\n",
    "plt.title(\"$\\kappa_{rbf}(x_i,x_j)$\", size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same now for one of our real time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-series for sub-indicator 'SI_POV_DAY1' in Algeria\n",
    "X = np.array(dict_all_i['Algeria'][period].loc['SI_POV_DAY1']).reshape((-1, 1))\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "# ==== k(x,0)\n",
    "\n",
    "plt.subplot(121) # left plot\n",
    "\n",
    "# First, sample kernel at x_j = 0\n",
    "k.lengthscale = 1\n",
    "K = k.K(X, np.array([[0.]])) # k(x,0)\n",
    "\n",
    "# Plot covariance vector\n",
    "plt.plot(X,K)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_i$\", size=20), plt.ylabel(\"$\\kappa$\", size=20)\n",
    "plt.title(\"$\\kappa_{rbf}(x_i,0)$\", size=20)\n",
    "\n",
    "# ==== k(x_i,x_j)\n",
    "\n",
    "plt.subplot(122) # right plot\n",
    "\n",
    "# The kernel takes two inputs, and outputs the covariance between each respective point in the two inputs\n",
    "K = k.K(X,X)\n",
    "\n",
    "# Plot the covariance of the sample space\n",
    "plt.pcolor(X.T, X, K)\n",
    "\n",
    "# Format and annotate plot\n",
    "plt.gca().invert_yaxis(), plt.gca().axis(\"image\")\n",
    "plt.xlim(int(min(X)), int(max(X))), plt.ylim(int(max(X)), int(min(X)))\n",
    "plt.xlabel(\"$x_i$\", size=20), plt.ylabel(\"$x_j$\", size=20), plt.colorbar()\n",
    "plt.title(\"$\\kappa_{rbf}(x_i,x_j)$\", size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the kernel parameters\n",
    "\n",
    "Looking at the above definition of the RBF kernel, we can see that the parameters, i.e. variance and lengthscale, control the shape of the covariance function and therefore the value of the covariance between points $x_i$ and $x_j$.\n",
    "\n",
    "We can access the value of the kernel parameters in `GPy` and manually set them by calling `k.lengthscale` or `k.variance` for the RBF kernel. The following example demonstrates how the value of the lengthscale affects the RBF kernel. \n",
    "\n",
    "We only do this with our too-perfect-for-reality toy data set, because it should just demonstrate how influential these hyperparameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our toy data set and sample space: 250 samples in the interval [-4,4] \n",
    "X = np.linspace(-4.,4.,250)[:, None] # we use more samples to get a smoother plot at low lengthscales\n",
    "\n",
    "# Create a 1-D RBF kernel with default parameters\n",
    "k = GPy.kern.RBF(1)\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "# Set up our list of different lengthscales\n",
    "ls = [0.25, 0.5, 1., 2., 4.]\n",
    "\n",
    "# Loop over the lengthscale values\n",
    "for l in ls:\n",
    "    # Set the lengthscale to be l\n",
    "    k.lengthscale = l\n",
    "    # Calculate the new covariance function at k(x_i,0)\n",
    "    C = k.K(X, np.array([[0.]]))\n",
    "    # Plot the resulting covariance vector\n",
    "    plt.plot(X,C)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_i$\", size=20), plt.ylabel(\"$\\kappa(x_i,0)$\", size=20) \n",
    "plt.title(\"Effects of different lengthscales on the Gaussian RBF kernel\", size=16)\n",
    "plt.legend(labels=ls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP regression model with toy data set\n",
    "\n",
    "We will now use our Gaussian process prior, i.e. we assume all measurements come from an univariate Gaussian distribution, to learn the Gaussian process posterior with our actual measurements to form a GP regression model. \n",
    "\n",
    "As above, we do this with **toy data set** first and then with our real data.\n",
    "\n",
    "The toy data set changes from the examples from above. We assume our underlying function $f(x)$ has this form:\n",
    "$$\n",
    "    f(x) = -\\cos(2\\pi x) + \\frac{1}{2}\\sin(6\\pi x) ,\n",
    "$$\n",
    "but our actual measurements $y$ of this function $f(x)$ are somewhat noisy, i.e. \n",
    "$$\n",
    "    y = f(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.01).\n",
    "$$\n",
    "\n",
    "\n",
    "Let's have a first view on this toy data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function, call f(x) to generate data\n",
    "f = lambda x: -np.cos(2*np.pi*x) + 0.5*np.sin(6*np.pi*x)\n",
    "\n",
    "# 10 equally spaced sample locations \n",
    "X = np.linspace(0.05, 0.95, 10)[:,None]\n",
    "\n",
    "# y = f(X) + epsilon\n",
    "Y = f(X) + np.random.normal(0., 0.1, (10,1)) # note that np.random.normal takes mean and s.d. (not variance), 0.1^2 = 0.01\n",
    "\n",
    "# Setup our figure environment\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "# Plot observations\n",
    "plt.plot(X, Y, \"kx\", mew=2)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"x\"), plt.ylabel(\"f\")\n",
    "plt.legend(labels=[\"sample points\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian process regression model using a Gaussian RBF covariance function can be defined first by setting up the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(1, variance=1., lengthscale=0.1, name=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then combining it with the data to form a Gaussian process regression model, with $\\mathbf{X}^*$ representing _any_ new inputs (imagine $\\mathbf{f}^*$ approximates $f(\\mathbf{X}^*)$):\n",
    "\n",
    "$$\n",
    "\\left.\\mathbf{f}^*\\,\\right|\\,\\mathbf{X}^*,\\mathbf{X},\\mathbf{y} \\sim \\mathcal{N}\\left(\\mathbf{m}, \\mathbf{C}\\right),\n",
    "$$\n",
    "\n",
    "where $\n",
    "\\mathbf{m} = \\mathbf{K}_{*x}(\\mathbf{K}_{xx} + \\sigma^2\\mathbf{I})^{-1}\\mathbf{y}$, \n",
    "\n",
    "$\\mathbf{C} = \\mathbf{K}_{**} -  \\mathbf{K}_{*x}(\\mathbf{K}_{xx} + \\sigma^2\\mathbf{I})^{-1}\\mathbf{K}_{*x}^\\text{T}\n",
    "$, \n",
    "\n",
    "and covariance matrices are defined by evaluations of the kernel functions: \n",
    "\n",
    "$\\mathbf{K}_{xx} = k(\\mathbf{X}, \\mathbf{X})$; $\\mathbf{K}_{*x} = k(\\mathbf{X}^*, \\mathbf{X})$; and $\\mathbf{K}_{**} = k(\\mathbf{X}^*,\\mathbf{X}^*)$.\n",
    "\n",
    "The following cell is a script which can also be done automatically with `GPy.models.GPRegression(X, Y, k)`, see a couple of cells down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test points to sample function from\n",
    "Xnew = np.linspace(-0.05, 1.05, 100)[:, None]\n",
    "\n",
    "# Covariance between training sample points (+ Gaussian noise)\n",
    "Kxx = k.K(X,X) + 1 * np.eye(10)\n",
    "\n",
    "# Covariance between training and test points\n",
    "Kxs = k.K(Xnew, X)\n",
    "\n",
    "# Covariance between test points\n",
    "Kss = k.K(Xnew,Xnew)\n",
    "\n",
    "# The mean of the GP fit (note that @ is matrix multiplcation: A @ B is equivalent to np.matmul(A,B))\n",
    "mean = Kxs @ np.linalg.inv(Kxx) @ Y\n",
    "# The covariance matrix of the GP fit\n",
    "Cov = Kss - Kxs @ np.linalg.inv(Kxx) @ Kxs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a quick plotting utility function for our GP fits. There are a number of plotting options available in GPy, but we will use the below method, which plots the mean and $95\\%$ confidence fit of a GP for a given input $\\mathbf{X}^*$. Optionally, we will allow it to plot the initial training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(X, m, C, training_points=None):\n",
    "    \"\"\" Plotting utility to plot a GP fit with 95% confidence interval \"\"\"\n",
    "    # Plot 95% confidence interval \n",
    "    plt.fill_between(X[:,0],\n",
    "                     m[:,0] - 1.96*np.sqrt(np.diag(C)),\n",
    "                     m[:,0] + 1.96*np.sqrt(np.diag(C)),\n",
    "                     alpha=0.5)\n",
    "    # Plot GP mean and initial training points\n",
    "    plt.plot(X, m, \"-\")\n",
    "    plt.legend(labels=[\"GP fit\"])\n",
    "    \n",
    "    plt.xlabel(\"years\", size=20), plt.ylabel(\"standardised values\", size=20)\n",
    "    \n",
    "    # Plot training points if included\n",
    "    if training_points is not None:\n",
    "        X_, Y_ = training_points\n",
    "        plt.plot(X_, Y_, \"kx\", mew=2)\n",
    "        plt.legend(labels=[\"GP fit\", \"sample points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot the GP fit mean and covariance\n",
    "plot_gp(Xnew, mean, Cov, training_points=(X,Y))\n",
    "plt.title(\"Explicit (homemade) regression model fit\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save effort and time by to do Gaussian process regression using `GPy`, by creating a GP regression model with sample points $(\\mathbf{X}, \\mathbf{Y})$ and the Gaussian RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPy.models.GPRegression(X, Y, k)\n",
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use GPy's regression and prediction tools, which _should_ give the same result as our basic implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPy model to calculate the mean and covariance of the fit at Xnew\n",
    "mean, Cov = m.predict_noiseless(Xnew, full_cov=True)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot the GP fit mean and covariance\n",
    "plot_gp(Xnew, mean, Cov, training_points=(X,Y))\n",
    "plt.title(\"GPy regression model fit\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance function parameter estimation\n",
    "\n",
    "The values of kernel parameters can be estimated by maximising the likelihood of the observations. This is useful to optimise our estimate of the underlying function, without eye-balling parameters to get a good fit.\n",
    "\n",
    "In `GPy`, the `model` objects such as `GPRegression`, have parameter optimisation functionality. We can call this as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the selected parameters in the model table above. The regression fit with the optimised parameters can be plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean and covariance of optimised GP\n",
    "mean, Cov = m.predict_noiseless(Xnew, full_cov=True)\n",
    "\n",
    "# Setup the figure environment\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot the GP fit mean and covariance\n",
    "plot_gp(Xnew, mean, Cov, training_points=(X,Y))\n",
    "plt.plot(Xnew, f(Xnew), \"r:\", lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter constraints\n",
    "\n",
    "We can see in the above model that the regression model is fit to the data, as the optimiser has minimised the noise effect in the model, `Gaussian_noise.variance`$ = 1.71\\times10^{-8}$. If we *know*, or can reasonably approximate, the variance of the observation noise $\\epsilon$, we can fix this parameter for the optimiser, using `fix`, which in the case of the above is $0.001$. We can also limit the values that the parameters take by adding constraints. For example, the variance and lengthscale can only be positive, so calling `constrain_positive`, we can enforce this (note that this is the default constraint for GP regression anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrain the regression parameters to be positive only\n",
    "m.constrain_positive()\n",
    "\n",
    "# Fix the Gaussian noise variance at 0.0001 \n",
    "m.Gaussian_noise.variance = 0.001 # (Reset the parameter first)\n",
    "m.Gaussian_noise.variance.fix()\n",
    "\n",
    "#m.rbf.variance = 0.5\n",
    "#m.rbf.variance.fix()\n",
    "\n",
    "# Reoptimise\n",
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our constraints in the corresponding column in the above table, where \"`+ve`\" means we are constrained to positive values, and `fixed` means the optimiser will not try and optimise this parameter. We can see here that the variance of the noise in the model is unchanged by the optimiser. Looking at the resulting plot, we can see that we have a much more reasonable confidence in our estimate, and that the true function is hard to distinguish from samples drawn from our fit, indicating that we have reasonable approximation of the true function given noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean and covariance of optimised GP\n",
    "mean, Cov = m.predict_noiseless(Xnew, full_cov=True)\n",
    "\n",
    "# Setup our figure environment\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "# The top plot shows our mean regression fit and 95% confidence intervals \n",
    "plt.subplot(211)\n",
    "# Plot the GP fit mean and covariance\n",
    "plot_gp(Xnew, mean, Cov, training_points=(X,Y))\n",
    "plt.title(\"GP posterior\")\n",
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(Xnew, f(Xnew),\"r:\", lw=3)\n",
    "\n",
    "Z  = np.random.multivariate_normal(mean[:,0], Cov, 20)\n",
    "for z in Z:\n",
    "    plt.plot(Xnew,z, \"g-\", alpha=0.2)\n",
    "    \n",
    "plt.xlabel(\"x\"), plt.ylabel(\"f\")\n",
    "plt.legend(labels=[\"true $f(x)$\", \"samples from GP\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP regression model with SDG data set\n",
    "\n",
    "Now, we'll implement everything from above with our SDG data set. We have already plotted it above, but will still do it here again to keep it in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years from 2000 to 2018\n",
    "X = np.array(per).reshape((-1, 1))\n",
    "\n",
    "# standardised values for these X years\n",
    "Y = np.array(dict_all_i['Algeria'][period].loc['SI_POV_DAY1']).reshape((-1, 1)) + np.random.normal(0., 0.1, (19,1))\n",
    "\n",
    "# Setup our figure environment\n",
    "plt.figure(figsize=(19, 7))\n",
    "\n",
    "# Plot observations\n",
    "plt.plot(X, Y, \"kx\", mew=2)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"years\", size=20), plt.ylabel(\"standardised values\", size=20)\n",
    "plt.xticks(np.arange(min(X), max(X)+1, 1.0))\n",
    "plt.legend(labels=[\"measurements\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel\n",
    "k = GPy.kern.RBF(1, variance=1., lengthscale=0.1, name=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise kernel\n",
    "# Constrain the regression parameters to be positive only\n",
    "m.constrain_positive()\n",
    "\n",
    "# Fix the Gaussian noise variance at 0.0001 \n",
    "m.Gaussian_noise.variance = 0.001 # (Reset the parameter first)\n",
    "m.Gaussian_noise.variance.fix()\n",
    "\n",
    "#m.rbf.variance = 0.5\n",
    "#m.rbf.variance.fix()\n",
    "\n",
    "# optimise\n",
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the short paths and make the regression with `GPy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPy.models.GPRegression(X, Y, k)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `GPy`'s prediction tool for the years and months of years which we do not have any data for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test points to sample function from\n",
    "Xnew = np.linspace(1995, 2023, 100)[:, None]\n",
    "\n",
    "# Use GPy model to calculate the mean and covariance of the fit at Xnew\n",
    "mean, Cov = m.predict_noiseless(Xnew, full_cov=True)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot the GP fit mean and covariance\n",
    "plot_gp(Xnew, mean, Cov, training_points=(X,Y))\n",
    "plt.xticks(np.arange(min(Xnew), max(Xnew)+1, 1.0))\n",
    "plt.title(\"GPy regression model fit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "The dataset provided is separated automatically into a training and test set. We will use the training set to fit our GP and use the test data to visualise the predictive power of our GP fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (X = input, Y = observation)\n",
    "X, Y = data['X'], data['Y']\n",
    "\n",
    "# Test data (Xtest = input, Ytest = observations)\n",
    "Xtest, Ytest = data['Xtest'], data['Ytest']\n",
    "\n",
    "# Set up our plotting environment\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot the training data in blue and the test data in red\n",
    "plt.plot(X, Y, \"b.\", Xtest, Ytest, \"r.\")\n",
    "\n",
    "# Annotate plot\n",
    "plt.legend(labels=[\"training data\", \"test data\"])\n",
    "plt.xlabel(\"year\"), plt.ylabel(\"CO$_2$ (PPM)\"), plt.title(\"Monthly mean CO$_2$ at the Mauna Loa Observatory, Hawaii\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP regression\n",
    "\n",
    "First, we will try to fit a basic RBF to our data, as we have used in previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(1, name=\"rbf\")\n",
    "\n",
    "m = GPy.models.GPRegression(X, Y, k)\n",
    "m.optimize()\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.vstack([X, Xtest])\n",
    "\n",
    "mean, Cov = m.predict(Xnew, full_cov=True)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_gp(Xnew, mean, Cov)\n",
    "plt.plot(X, Y, \"b.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the plots now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, Cov = m.predict(Xnew, full_cov=True)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# The left plot shows the GP fit to a subsample of our training set\n",
    "plt.subplot(121)\n",
    "plot_gp(Xnew, mean, Cov)\n",
    "plt.plot(X, Y, \"b.\");\n",
    "plt.gca().set_xlim([1960,1980]), plt.gca().set_ylim([310, 340])\n",
    "\n",
    "# The right plot shows that the GP has no predictive power and reverts to 0\n",
    "plt.subplot(122)\n",
    "plot_gp(Xnew, mean, Cov)\n",
    "plt.plot(X, Y, \"b.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
