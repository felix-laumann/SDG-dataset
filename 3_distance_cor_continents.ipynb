{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear dependencies amongst the SDGs and climate change by distance correlation\n",
    "\n",
    "We start with investigating dependencies amongst the SDGs on different levels. The method how we investigate these dependencies should take as few assumptions as possible. So, a Pearson linear correlation coefficient or a rank correlation coefficient are not our choice since they assume linearity and/or monotony, respectively.\n",
    "\n",
    "We choose to compute the [distance correlation](https://projecteuclid.org/euclid.aos/1201012979), precisely the [partial distance correlation](https://projecteuclid.org/download/pdfview_1/euclid.aos/1413810731), because of the following properties:\n",
    "1. we have an absolute measure of dependence ranging from $0$ to $1$, $0 \\leq \\mathcal{R}(X,Y) \\leq 1$\n",
    "2. $\\mathcal{R}(X,Y) = 0$ if and only if $X$ and $Y$ are independent,\n",
    "3. $\\mathcal{R}(X,Y) = \\mathcal{R}(Y,X)$\n",
    "4. we are able to investigate non-linear and non-monotone relationships,\n",
    "5. we can find dependencies between indicators with differently many measurements,\n",
    "6. the only assumptions we need to take is that probability distributions have finite first moments.\n",
    "\n",
    "The conditional distance correlation has the advantage that we ignore the influence of any other targets or goals when we compute the correlation between any two targets or goals. This procedure is also called controlling for confounders.\n",
    "\n",
    "The **distance correlation** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}^2(X,Y) = \\begin{cases}\n",
    "\\frac{\\mathcal{V}^2 (X,Y)}{\\sqrt{\\mathcal{V}^2 (X)\\mathcal{V}^2 (Y)}} &\\text{, if $\\mathcal{V}^2 (X)\\mathcal{V}^2 (Y) > 0$} \\\\\n",
    "0 &\\text{, if $\\mathcal{V}^2 (X)\\mathcal{V}^2 (Y) = 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{V}^2 (X,Y) = \\| f_{X,Y}(t) - f_X(t)f_Y(t) \\|^2\n",
    "$$\n",
    "\n",
    "\n",
    "is the distance covariance with **characteristic functions** $f(t)$. Bear in mind that characteristic functions include the imaginary unit $i$, $i^2 = -1$:\n",
    "\n",
    "$$\n",
    "f_X(t) = \\mathbb{E}[e^{itX}]\n",
    "$$\n",
    "\n",
    "Thus, we are in the space of complex numbers $\\mathbb{C}$. Unfortunately, this means we can most likely not find exact results, but we'll get back to this later under Estimators.\n",
    "\n",
    "The **conditional distance correlation** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}^2(X,Y \\ | \\ Z) = \\begin{cases}\n",
    "\\frac{\\mathcal{R}^2 (X,Y) - \\mathcal{R}^2 (X,Z) \\mathcal{R}^2 (Y,Z)}{\\sqrt{1 - \\mathcal{R}^4 (X,Z)} \\sqrt{1 - \\mathcal{R}^4 (Y,Z)}} &\\text{, if $\\mathcal{R}^4 (X,Z) \\neq 1$ and $\\mathcal{R}^4 (Y,Z) \\neq 1$} \\\\\n",
    "0 &\\text{, if $\\mathcal{R}^4 (X,Z) = 1$ and $\\mathcal{R}^4 (Y,Z) = 1$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "# Distance covariance\n",
    "Let's dismantle the distance covariance equation to know what we actually compute in the distance correlation:\n",
    "\n",
    "$$\n",
    "\\mathcal{V}^2 (X,Y) = \\| f_{X,Y}(t) - f_X(t) \\ f_Y(t) \\|^2 = \\frac{1}{c_p c_q} \\int_{\\mathbb{R}^{p+q}} \\frac{| f_{X,Y}(t) - f_X(t)f_Y(t) |^2}{| t |_p^{1+p} \\ | t |_q^{1+q}} dt\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "c_d = \\frac{\\pi^{(1+d)/2}}{\\Gamma \\Big( (1+d)/2 \\Big)}\n",
    "$$\n",
    "\n",
    "where the (complete) Gamma function $\\Gamma$ is\n",
    "\n",
    "$$\n",
    "\\Gamma (z) = \\int_0^{\\infty} x^{z-1} \\ e^{-x} \\ dx\n",
    "$$\n",
    "\n",
    "with $z \\in \\mathbb{R}^{+}$. \n",
    "\n",
    "$p$ and $q$ are the samples of time-series. We can see this as a random vector with multiple samples available for each time point. However, the number of samples for time points must not vary over the same time-series. We can write this as: \n",
    "\n",
    "$$X \\ \\text{in} \\ \\mathbb{R}^p$$\n",
    "\n",
    "$$Y \\ \\text{in} \\ \\mathbb{R}^q$$\n",
    "\n",
    "\n",
    "A preliminary conclusion of this formulation: **we can compute dependencies between time-series with different numbers of samples**. \n",
    "\n",
    "But we still have some terms in the distance covariance $\\mathcal{V}^2 (X,Y)$ which we need to define:\n",
    "\n",
    "$ | t |_p^{1+p} $ is the Euclidean distance of $t$ in $\\mathbb{R}^p$, $ | t |_q^{1+q} $ is the Euclidean distance of $t$ in $\\mathbb{R}^q$.\n",
    "\n",
    "The numerator in the integral of $\\mathcal{V}^2 (X,Y)$ is:\n",
    "$$\n",
    "| f_{X,Y}(t) - f_X(t) \\ f_Y(t) |^2 = \\Big( 1- |f_X(t) | ^2 \\Big) \\ \\Big( 1- |f_Y(t) |^2 \\Big)\n",
    "$$\n",
    "\n",
    "where $|f_X(t) |$ and $|f_Y(t) |$ are absolute random vectors of the characteristic functions $f(t)$ with $p$ and $q$ samples, respectively.\n",
    "\n",
    "\n",
    "## Estimators\n",
    "\n",
    "Since the characteristic functions include the imaginary unit $i$, we cannot recover the exact solution for the distance covariance. However, we can estimate it by a quite simple form. We compute these estimators according to [Huo & Szekely, 2016](https://arxiv.org/abs/1410.1503).\n",
    "\n",
    "We denote the pairwise distances of the $X$ observations by $a_{ij} := \\|X_i - X_j \\|$ and of the $Y$ observations by $b_{ij} = \\|Y_i - Y_j \\|$ for $i,j = 1, ..., n$, where $n$ is the number of measurements in $X$ and $Y$. The corresponding distance matrices are denoted by $(A_{ij})^n_{i,j=1}$ and $(B_{ij})^n_{i,j=1}$, where\n",
    "\n",
    "$$\n",
    "A_{ij} = \\begin{cases}\n",
    "a_{ij} - \\frac{1}{n} \\sum_{l=1}^n a_{il} - \\frac{1}{n} \\sum_{k=1}^n a_{kj} + \\frac{1}{n^2} \\sum_{k,l=1}^n a_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "B_{ij} = \\begin{cases}\n",
    "b_{ij} - \\frac{1}{n} \\sum_{l=1}^n b_{il} - \\frac{1}{n} \\sum_{k=1}^n b_{kj} + \\frac{1}{n^2} \\sum_{k,l=1}^n b_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Having computed these, we can estimate the sample distance covariance $\\hat{\\mathcal{V}}^2(X,Y)$ by\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{V}}^2(X,Y) = \\frac{1}{n^2} \\sum_{i,j=1}^n A_{ij} \\ B_{ij}\n",
    "$$\n",
    "\n",
    "The corresponding sample variance $\\hat{\\mathcal{V}}^2(X)$ is consequently:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{V}}^2(X) = \\frac{1}{n^2} \\sum_{i,j=1}^n A^2_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, we can scale these covariances to finally arrive at the sample distance correlation $\\hat{\\mathcal{R}}^2(X,Y)$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{R}}^2(X,Y) = \\begin{cases}\n",
    "\\frac{\\hat{\\mathcal{V}}^2 (X,Y)}{\\sqrt{\\hat{\\mathcal{V}}^2 (X)\\hat{\\mathcal{V}}^2 (Y)}} &\\text{, if $\\hat{\\mathcal{V}}^2 (X)\\mathcal{V}^2 (Y) > 0$} \\\\\n",
    "0 &\\text{, if $\\hat{\\mathcal{V}}^2 (X)\\hat{\\mathcal{V}}^2 (Y) = 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Unbiased estimators\n",
    "These estimators are biased, but we can define unbiased estimators of the distance covariance $\\hat{\\mathcal{V}}^2(X,Y)$ and call them $\\Omega_n(x,y)$. We must first redefine our distance matrices $(A_{ij})^n_{i,j=1}$ and $(B_{ij})^n_{i,j=1}$, which we will call $(\\tilde{A}_{ij})^n_{i,j=1}$ and $(\\tilde{B}_{ij})^n_{i,j=1}$:\n",
    "\n",
    "$$\n",
    "\\tilde{A}_{ij} = \\begin{cases}\n",
    "a_{ij} - \\frac{1}{n-2} \\sum_{l=1}^n a_{il} - \\frac{1}{n-2} \\sum_{k=1}^n a_{kj} + \\frac{1}{(n-1)(n-2)} \\sum_{k,l=1}^n a_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\tilde{B}_{ij} = \\begin{cases}\n",
    "b_{ij} - \\frac{1}{n-2} \\sum_{l=1}^n b_{il} - \\frac{1}{n-2} \\sum_{k=1}^n b_{kj} + \\frac{1}{(n-1)(n-2)} \\sum_{k,l=1}^n b_{kl} & i \\neq j; \\\\\n",
    "0 & i = j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally, we can compute the unbiased estimator $\\Omega_n(X,Y)$ for $\\mathcal{V}^2(X,Y)$ as the dot product $\\langle \\tilde{A}, \\tilde{B} \\rangle$:\n",
    "\n",
    "$$\n",
    "\\Omega_n(X,Y) = \\langle \\tilde{A}, \\tilde{B} \\rangle = \\frac{1}{n(n-3)} \\sum_{i,j=1}^n \\tilde{A}_{ij} \\ \\tilde{B}_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "Interestingly, [Lyons (2013)](https://arxiv.org/abs/1106.5758) found another solution how not only the sample distance correlation can be computed, but also the population distance correlation without characteristic functions. This is good to acknowledge, but it is not necessary to focus on it. \n",
    "\n",
    "# Conditional distance covariance\n",
    "\n",
    "We start with computing the unbiased distance matrices $(\\tilde{A}_{ij})^n_{i,j=1}$, $(\\tilde{B}_{ij})^n_{i,j=1}$, and $(\\tilde{C}_{ij})^n_{i,j=1}$ for $X$, $Y$, and $Z$, respectively, as we have done previously for the distance covariance. We define the dot product\n",
    "\n",
    "$$\n",
    "\\Omega_n(X,Y) = \\langle \\tilde{A}, \\tilde{B} \\rangle = \\frac{1}{n(n-3)} \\sum_{i,j=1}^n \\tilde{A}_{ij} \\tilde{B}_{ij}\n",
    "$$\n",
    "\n",
    "and project the sample $x$ onto $z$ as \n",
    "\n",
    "$$\n",
    "P_z (x) = \\frac{\\langle \\tilde{A}, \\tilde{C} \\rangle}{\\langle \\tilde{C}, \\tilde{C} \\rangle} \\tilde{C} .\n",
    "$$\n",
    "\n",
    "The complementary projection is consequently\n",
    "\n",
    "$$\n",
    "P_{z^{\\bot}} (x) = \\tilde{A} - P_z (x) = \\tilde{A} - \\frac{\\langle \\tilde{A}, \\tilde{C} \\rangle}{\\langle \\tilde{C}, \\tilde{C} \\rangle} \\tilde{C} .\n",
    "$$\n",
    "\n",
    "Hence, the sample conditional distance covariance is\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{V}}^2(X,Y \\ | \\ Z) = \\langle P_{z^{\\bot}} (x), P_{z^{\\bot}} (y) \\rangle .\n",
    "$$\n",
    "\n",
    "Then, we can scale these covariances to finally arrive at the sample conditional distance correlation $\\hat{\\mathcal{R}}^2(X,Y \\ | \\ Z)$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{R}}^2(X,Y \\ | \\ Z) = \\begin{cases}\n",
    "\\frac{\\langle P_{z^{\\bot}} (x), P_{z^{\\bot}} (y) \\rangle}{\\| P_{z^{\\bot}} (x) \\| \\ \\| P_{z^{\\bot}} (y) \\|} &\\text{, if} \\ \\| P_{z^{\\bot}} (x) \\| \\ \\| P_{z^{\\bot}} (y) \\| \\neq 0 \\\\\n",
    "0 &\\text{, if} \\ \\| P_{z^{\\bot}} (x) \\| \\ \\| P_{z^{\\bot}} (y) \\| = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Implementation\n",
    "For our computations, we'll use the packages [`dcor`](https://dcor.readthedocs.io/en/latest/?badge=latest) for the partial distance correlation and [`community`](https://github.com/taynaud/python-louvain) for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dcor\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from community import community_louvain as community\n",
    "\n",
    "from dcor._dcor_internals import _u_distance_matrix, u_complementary_projection\n",
    "from sklearn.manifold import MDS\n",
    "import gc\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading standardised imputed data set\n",
    "We load first of all the standardised imputed data set which we have generated with the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_values_i = pickle.load(open('utils/data/targets_values_i_up_arr_wb.pkl', 'rb'))\n",
    "goals_values_i = pickle.load(open('utils/data/goals_values_i_up_arr_wb.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether T appended\n",
    "len(targets_values_i['Belgium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read amended csv file\n",
    "c = pd.read_csv('utils/countries_wb.csv', dtype=str, delimiter=';', header=None)\n",
    "countries = list(c[0])\n",
    "continents = pd.read_csv(r'utils/continents.csv')\n",
    "continents.replace({\"Democratic People's Republic of Korea\": \"Korea, Dem. People's Rep.\", 'Gambia': 'Gambia, The', 'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom', 'Congo': 'Congo, Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.', 'Czechia': 'Czech Republic', 'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'Kyrgyzstan': 'Kyrgyz Republic', \"Lao People's Democratic Republic\": 'Lao PDR', 'Republic of Moldova': 'Moldova', 'Micronesia (Federated States of)': 'Micronesia, Fed. Sts.', 'Slovakia': 'Slovak Republic', 'Viet Nam': 'Vietnam', 'Egypt': 'Egypt, Arab Rep.', 'United Republic of Tanzania': 'Tanzania','United States of America': 'United States', 'Venezuela (Bolivarian Republic of)': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.', 'Bahamas': 'Bahamas, The', 'Bolivia (Plurinational State of)': 'Bolivia'}, inplace=True)\n",
    "info = pd.read_csv(r'utils/wb_info.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes key in-place\n",
    "countries.remove('Micronesia, Fed. Sts.')\n",
    "continents['Oceania (excl. AUS + NZ)'] = continents['Oceania (excl. AUS + NZ)'].drop(index=4) # removing Micronesia\n",
    "continents['Oceania (incl. AUS + NZ)'] = continents['Oceania (incl. AUS + NZ)'].drop(index=6) # removing Micronesia\n",
    "continents['World'] = continents['World'].drop(index=170) # removing Micronesia\n",
    "continents.drop(['Northern Africa', 'Southern Africa', 'North America', 'Australia and New Zealand'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We later compute the correlations on an indicator level, but this is too detailed for any network visualisation and for an overarching understanding. Hence, we group here all sub-indicators first on an indicator-level. Then, we compute the distance correlations for the indicators, targets and goals.\n",
    "\n",
    "We work with the `info` file again, so we don't need to assign all of this by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "#targets_values_i['France'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have values for targets, so we must, first of all, generate a list of all unique **targets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(info[4].unique())\n",
    "\n",
    "dict_targets = {}\n",
    "\n",
    "for target in targets:\n",
    "    t = info[0].where(info[4] == target)\n",
    "\n",
    "    dict_targets[target] = [i for i in t if str(i) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check \n",
    "dict_targets['1.2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we also generate a list of all unique **goals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = list(info[3].unique())\n",
    "\n",
    "dict_goals = {}\n",
    "\n",
    "for goal in goals:\n",
    "    g = info[4].where(info[3] == goal)\n",
    "\n",
    "    dict_goals[goal] = [t for t in g if str(t) != 'nan']\n",
    "    dict_goals[goal] = list(set(dict_goals[goal]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check \n",
    "print(dict_goals['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance correlations between goals\n",
    "\n",
    "The next step is to compute the distance correlations on a goal-level.\n",
    "\n",
    "We work with the **concatenated time-series** to compute the conditioned distance correlation directly on goal-level data. Visually speaking, this means that we fit one non-linear function to the data for all targets of these two goals. Since goals often have diverse targets, this may end up in fitting a non-linear curve to very noisy data.\n",
    "\n",
    "## Working with concatenated time-series\n",
    "\n",
    "### Conditioning iteratively on subsets of joint distributions of all goals\n",
    "We condition pairs of two goals iteratively on subsets of all remaining goals. We start with conditioning on the empty set, i.e. we compute the pairwise distance correlation first. Afterwards, we increase the set to condition on until we have reached the set of all remaining 15 goals to condition on. These sets are represented by the joint distributions of the goals entailed in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to condition on all **subsets** of these lists of SDGs we condition on to find the dependence which solely stems from either of the two SDGs we condition the others on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(iterable, r):\n",
    "    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "    # combinations(range(4), 3) --> 012 013 023 123\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield list(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i+1, r):\n",
    "            indices[j] = indices[j-1] + 1\n",
    "        yield list(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_tuple(iterable, r):\n",
    "    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "    # combinations(range(4), 3) --> 012 013 023 123\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield tuple(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i+1, r):\n",
    "            indices[j] = indices[j-1] + 1\n",
    "        yield tuple(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(pool_0, pool_1):\n",
    "    result = [[x, y]+[z] for x, y in pool_0 for z in pool_1]    # ~ 40 Mio rows\n",
    "    for prod in result:\n",
    "        yield tuple(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list out of all unique combinations of goals\n",
    "g_combinations = list(combinations(goals, 2))\n",
    "conditions_g = []\n",
    "conditions_g_tuple = []\n",
    "for i in range(1, 18):\n",
    "    conditions_g.extend(list(combinations(goals, i)))\n",
    "    conditions_g_tuple.extend(tuple(combinations_tuple(goals, i)))\n",
    "\n",
    "# divide conditions_g_tuple into four sub-lists to save memory\n",
    "conditions_g_tuple_1 = conditions_g_tuple[:int(len(conditions_g_tuple)/4)]\n",
    "conditions_g_tuple_2 = conditions_g_tuple[int(len(conditions_g_tuple)/4)+1:2*int(len(conditions_g_tuple)/4)]\n",
    "conditions_g_tuple_3 = conditions_g_tuple[2*int(len(conditions_g_tuple)/4)+1:3*int(len(conditions_g_tuple)/4)]\n",
    "conditions_g_tuple_4 = conditions_g_tuple[3*int(len(conditions_g_tuple)/4)+1:]\n",
    "    \n",
    "pairs = list(product(g_combinations, conditions_g_tuple))\n",
    "pairs_g0 = pd.DataFrame.from_records(pairs, columns=['pair_0', 'pair_1', 'condition'])\n",
    "\n",
    "pairs_1 = list(product(g_combinations, conditions_g_tuple_1))\n",
    "pairs_g0_1 = pd.DataFrame.from_records(pairs_1, columns=['pair_0', 'pair_1', 'condition'])\n",
    "pairs_2 = list(product(g_combinations, conditions_g_tuple_2))\n",
    "pairs_g0_2 = pd.DataFrame.from_records(pairs_2, columns=['pair_0', 'pair_1', 'condition'])\n",
    "pairs_3 = list(product(g_combinations, conditions_g_tuple_3))\n",
    "pairs_g0_3 = pd.DataFrame.from_records(pairs_3, columns=['pair_0', 'pair_1', 'condition'])\n",
    "pairs_4 = list(product(g_combinations, conditions_g_tuple_4))\n",
    "pairs_g0_4 = pd.DataFrame.from_records(pairs_4, columns=['pair_0', 'pair_1', 'condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many rows?\n",
    "print(len(pairs_g0))\n",
    "print(len(pairs_g0_1), len(pairs_g0_2), len(pairs_g0_3), len(pairs_g0_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding empty condition set for pairwise dcor\n",
    "pairs_g1 = pd.DataFrame.from_records(data=g_combinations, columns=['pair_0', 'pair_1'])\n",
    "pairs_g1['condition'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "continents_prep_g = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    continents_prep_g[continent] = np.empty(18, dtype=object)\n",
    "    \n",
    "    for g, goal in enumerate(goals):\n",
    "        g_list = []\n",
    "        for country in continents[continent].dropna():\n",
    "            g_list.append(np.asarray(goals_values_i[country][g]))\n",
    "\n",
    "        continents_prep_g[continent][g] = np.asarray(g_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call these data in our `dcor` computations. We first compute the pairwise distance covariance and correlation, then the partial ones with conditioning on all the previously defined sets in `pairs_g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "Filtering out the conditions that contain goals $X$ (`pair_0`) or $Y$ (`pair_1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "pairs_g0_left_0 = pd.read_csv('utils/pairs_g0_left_0.zip', dtype=str, compression='zip')\n",
    "\n",
    "pairs_g0_left_0_1 = pd.read_csv('utils/pairs_g0_left_0_1.zip', dtype=str, compression='zip')\n",
    "pairs_g0_left_0_2 = pd.read_csv('utils/pairs_g0_left_0_2.zip', dtype=str, compression='zip')\n",
    "pairs_g0_left_0_3 = pd.read_csv('utils/pairs_g0_left_0_3.zip', dtype=str, compression='zip')\n",
    "pairs_g0_left_0_4 = pd.read_csv('utils/pairs_g0_left_0_4.zip', dtype=str, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "pairs_g0_left_0_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only keep rows which do not have an SDG in their condition\n",
    "pairs_g0_left = []\n",
    "\n",
    "pairs_g0_left_1 = []\n",
    "pairs_g0_left_2 = []\n",
    "pairs_g0_left_3 = []\n",
    "pairs_g0_left_4 = []\n",
    "\n",
    "for i in tqdm(pairs_g0.index):\n",
    "        if (pairs_g0.loc[i, 'pair_0'] not in pairs_g0.loc[i, 'condition'] and pairs_g0.loc[i, 'pair_1'] not in pairs_g0.loc[i, 'condition']):\n",
    "            pairs_g0_left.append(i)\n",
    "\n",
    "for i in tqdm(pairs_g0_1.index):\n",
    "        if (pairs_g0_1.loc[i, 'pair_0'] not in pairs_g0_1.loc[i, 'condition'] and pairs_g0_1.loc[i, 'pair_1'] not in pairs_g0_1.loc[i, 'condition']):\n",
    "            pairs_g0_left_1.append(i)\n",
    "\n",
    "for i in tqdm(pairs_g0_2.index):\n",
    "        if (pairs_g0_2.loc[i, 'pair_0'] not in pairs_g0_2.loc[i, 'condition'] and pairs_g0_2.loc[i, 'pair_1'] not in pairs_g0_2.loc[i, 'condition']):\n",
    "            pairs_g0_left_2.append(i)\n",
    "            \n",
    "for i in tqdm(pairs_g0_3.index):\n",
    "        if (pairs_g0_3.loc[i, 'pair_0'] not in pairs_g0_3.loc[i, 'condition'] and pairs_g0_3.loc[i, 'pair_1'] not in pairs_g0_3.loc[i, 'condition']):\n",
    "            pairs_g0_left_3.append(i)\n",
    "            \n",
    "for i in tqdm(pairs_g0_4.index):\n",
    "        if (pairs_g0_4.loc[i, 'pair_0'] not in pairs_g0_4.loc[i, 'condition'] and pairs_g0_4.loc[i, 'pair_1'] not in pairs_g0_4.loc[i, 'condition']):\n",
    "            pairs_g0_left_4.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows are left?\n",
    "print(len(pairs_g0_left_1), len(pairs_g0_left_2), len(pairs_g0_left_3), len(pairs_g0_left_4))\n",
    "\n",
    "pairs_g0_left_0_1 = pairs_g0_1.iloc[pairs_g0_left_1]\n",
    "pairs_g0_left_0_2 = pairs_g0_2.iloc[pairs_g0_left_2]\n",
    "pairs_g0_left_0_3 = pairs_g0_3.iloc[pairs_g0_left_3]\n",
    "pairs_g0_left_0_4 = pairs_g0_4.iloc[pairs_g0_left_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "pairs_g0_left_0_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pairs_g0_left_0's\n",
    "compression_opts = dict(method='zip', archive_name='pairs_g0_left_0.csv') \n",
    "pairs_g0_left_0.to_csv('utils/pairs_g0_left_0.zip', index=False, compression=compression_opts)\n",
    "\n",
    "compression_opts = dict(method='zip', archive_name='pairs_g0_left_0_1.csv') \n",
    "pairs_g0_left_0_1.to_csv('utils/pairs_g0_left_0_1.zip', index=False, compression=compression_opts)\n",
    "\n",
    "compression_opts = dict(method='zip', archive_name='pairs_g0_left_0_2.csv') \n",
    "pairs_g0_left_0_2.to_csv('utils/pairs_g0_left_0_2.zip', index=False, compression=compression_opts)\n",
    "\n",
    "compression_opts = dict(method='zip', archive_name='pairs_g0_left_0_3.csv') \n",
    "pairs_g0_left_0_3.to_csv('utils/pairs_g0_left_0_3.zip', index=False, compression=compression_opts)\n",
    "\n",
    "compression_opts = dict(method='zip', archive_name='pairs_g0_left_0_4.csv') \n",
    "pairs_g0_left_0_4.to_csv('utils/pairs_g0_left_0_4.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With  `multiprocessing`  parallelisation\n",
    "\n",
    "\n",
    " \n",
    "### Partial distance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('distance_cor'):\n",
    "    os.mkdir('distance_cor')\n",
    "    \n",
    "if not os.path.exists('distance_cor/goals'):\n",
    "    os.mkdir('distance_cor/goals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_distance_cor(row):\n",
    "    pair_0, pair_1, cond = row\n",
    "    if pair_0=='T':\n",
    "        pair_0 = 18\n",
    "    if pair_1=='T':\n",
    "        pair_1 = 18\n",
    "    pair_0_array = continents_prep_g[continent][int(pair_0)-1]\n",
    "    pair_1_array = continents_prep_g[continent][int(pair_1)-1]\n",
    "    condition_array = conditions_dict[str(cond)].T\n",
    "    \n",
    "    return dcor.partial_distance_correlation(pair_0_array, pair_1_array, condition_array)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-list of regions containing continents\n",
    "regions = {'Africa': ['Eastern Africa', 'Middle Africa', 'Western Africa', 'Sub-Saharan Africa', 'Africa'], \n",
    "           'Americas': ['Caribbean', 'Central America', 'South America', 'Latin America and the Caribbean', 'Americas'], \n",
    "           'Asia': ['Central and Eastern Asia', 'South-eastern Asia', 'Southern Asia', 'Western Asia', 'Asia'], \n",
    "           'Europe': ['Eastern Europe', 'Northern Europe', 'Southern Europe', 'Western Europe', 'Europe'], \n",
    "           'Oceania': ['Oceania (excl. AUS + NZ)', 'Oceania (incl. AUS + NZ)']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {'World': ['World']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "\n",
    "for region in regions:   # to save memory\n",
    "    print(region)\n",
    "    \n",
    "    dict_cor_goals_continents_2_cond = {}\n",
    "\n",
    "    for continent in regions[region]:\n",
    "        print(continent)\n",
    "\n",
    "        dict_cor_goa_c = pairs_g0_left_0.copy(deep=True)\n",
    "        #dict_cor_goa_c = pairs_g0_left_0_4.copy(deep=True)    # pairs_g0_left_0 has all non-empty conditional sets\n",
    "\n",
    "        # preparing conditional set\n",
    "        conditions_dict = {}\n",
    "\n",
    "        for cond in tqdm(conditions_g_tuple):\n",
    "        #for cond in conditions_g_tuple_4:\n",
    "            condition = []\n",
    "\n",
    "            for c in cond:                \n",
    "                if c=='T':\n",
    "                    condition.extend(continents_prep_g[continent][17].T)\n",
    "                else:\n",
    "                    condition.extend(continents_prep_g[continent][int(c)-1].T)\n",
    "\n",
    "            conditions_dict[str(cond)] = np.asarray(condition)\n",
    "\n",
    "        # partial distance correlation\n",
    "        pool = mp.Pool(int(mp.cpu_count()/2))\n",
    "\n",
    "        dict_cor_goa_c_list = dict_cor_goa_c.values.tolist()\n",
    "\n",
    "        print('start dcor...')\n",
    "\n",
    "        cor_results = pool.map(partial_distance_cor, dict_cor_goa_c_list, chunksize=1000)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        dict_cor_goa_c['dcor'] = cor_results\n",
    "\n",
    "        print('...dcor done')\n",
    "\n",
    "        # find minimum distance correlation between any two goals\n",
    "        dict_cor_goa_con = dict_cor_goa_c.groupby(['pair_0', 'pair_1'])['dcor'].apply(list).reset_index(name='list_dcor')\n",
    "\n",
    "        for i, row_c in dict_cor_goa_con.iterrows():\n",
    "            dict_cor_goa_con.loc[i, 'min_dcor_cond'] = min(dict_cor_goa_con.loc[i, 'list_dcor'])\n",
    "\n",
    "        dict_cor_goa_con.drop(columns=['list_dcor'], inplace=True)\n",
    "        \n",
    "        # finding conditional set of minimum partial distance correlation\n",
    "        dict_cor_goa_cond = dict_cor_goa_con.merge(dict_cor_goa_c, left_on='min_dcor_cond', right_on='dcor').drop(['pair_0_y', 'pair_1_y', 'dcor'], axis=1).rename(columns={'pair_0_x': 'pair_0', 'pair_1_x': 'pair_1'})\n",
    "    \n",
    "        dict_cor_goals_continents_2_cond[continent] = dict_cor_goa_cond\n",
    "        \n",
    "    # save every entry region separately to save memory\n",
    "    g_cor = open('distance_cor/goals/dict_cor_goals_continents_2_cond_{}.pkl'.format(region), 'wb')\n",
    "    #g_cor = open('distance_cor/goals/dict_cor_goals_continents_2_cond_{}_4.pkl'.format(region), 'wb')\n",
    "    pickle.dump(dict_cor_goals_continents_2_cond, g_cor)\n",
    "    g_cor.close()\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for World (disaggregated because of memory restrictions)\n",
    "dict_World_1 = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_World_1.pkl', 'rb'))\n",
    "dict_World_2 = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_World_2.pkl', 'rb'))\n",
    "dict_World_3 = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_World_3.pkl', 'rb'))\n",
    "dict_World_4 = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_World_4.pkl', 'rb'))\n",
    "\n",
    "cor_goals_continents_2_World = pd.concat([dict_World_1['World'], dict_World_2['World'], dict_World_3['World'], dict_World_4['World']])\n",
    "\n",
    "# find minimum distance correlation between any two goals\n",
    "dict_cor_goa_con = cor_goals_continents_2_World.groupby(['pair_0', 'pair_1'])['min_dcor_cond'].apply(list).reset_index(name='list_dcor')\n",
    "\n",
    "for i, row_c in dict_cor_goa_con.iterrows():\n",
    "    dict_cor_goa_con.loc[i, 'min_dcor_cond'] = min(dict_cor_goa_con.loc[i, 'list_dcor'])\n",
    "\n",
    "dict_cor_goa_con.drop(columns=['list_dcor'], inplace=True)\n",
    "\n",
    "# finding conditional set of minimum partial distance correlation\n",
    "dict_cor_goa_cond = dict_cor_goa_con.merge(cor_goals_continents_2_World, left_on='min_dcor_cond', right_on='min_dcor_cond').drop(['pair_0_y', 'pair_1_y'], axis=1).rename(columns={'pair_0_x': 'pair_0', 'pair_1_x': 'pair_1'})\n",
    "\n",
    "# save every entry region separately to save memory\n",
    "g_cor = open('distance_cor/goals/dict_cor_goals_continents_2_cond_World.pkl', 'wb')\n",
    "pickle.dump(dict_cor_goa_cond, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_Africa = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_Africa.pkl', 'rb'))\n",
    "dict_Americas = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_Americas.pkl', 'rb'))\n",
    "dict_Asia = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_Asia.pkl', 'rb'))\n",
    "dict_Europe = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_Europe.pkl', 'rb'))\n",
    "dict_Oceania = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_Oceania.pkl', 'rb'))\n",
    "dict_World = {}\n",
    "dict_World['World'] = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2_cond_World.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cor_goals_continents_2_condition = {**dict_Africa, **dict_Americas, **dict_Asia, **dict_Europe, **dict_Oceania, **dict_World}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(dict_cor_goals_continents_2_condition.keys())\n",
    "dict_cor_goals_continents_2_condition['Southern Europe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following continents which all have a sample size less or equal to 4 returned either NaNs or 1's. This is due to the assumption of a sample size greater or equal to 4 of the partial distance correlation.\n",
    "\n",
    "Northern Africa, Southern Africa, North America, Central Asia, Eastern Asia, Australia and New Zealand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_cor(row):\n",
    "    pair_0, pair_1 = row\n",
    "    if pair_0=='T':\n",
    "        pair_0 = 18\n",
    "    if pair_1=='T':\n",
    "        pair_1 = 18\n",
    "    pair_0_array = continents_prep_g[continent][int(pair_0)-1]\n",
    "    pair_1_array = continents_prep_g[continent][int(pair_1)-1]\n",
    "    \n",
    "    return dcor.distance_correlation(pair_0_array, pair_1_array)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "dict_cor_goals_continents_2_pair = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    dict_cor_goa_c_pair = pairs_g1.drop(columns=['condition']).copy(deep=True)     # pairs_g1 has empty conditional sets for pairwise dcor\n",
    "    \n",
    "    pool = mp.Pool(int(mp.cpu_count()/2))\n",
    "    \n",
    "    print('start dcor...')\n",
    "    \n",
    "    dict_cor_goa_c_pair_list = dict_cor_goa_c_pair.values.tolist()\n",
    "    \n",
    "    cor_results = pool.map(distance_cor, dict_cor_goa_c_pair_list, chunksize=1000)\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    dict_cor_goa_c_pair['min_dcor_pair'] = cor_results\n",
    "    \n",
    "    print('...dcor done')\n",
    "    \n",
    "    dict_cor_goals_continents_2_pair[continent] = dict_cor_goa_c_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dict_cor_goals_continents_2_pair['Europe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge dictionaries\n",
    "dict_cor_goals_continents_2 = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    \n",
    "    dict_cor_goals_continents_2[continent] = pd.DataFrame(index=range(153), columns=['pair_0', 'pair_1', 'min_dcor'])\n",
    "    \n",
    "    for i in dict_cor_goals_continents_2_pair[continent].index:\n",
    "        for j in dict_cor_goals_continents_2_condition[continent].index:\n",
    "            if dict_cor_goals_continents_2_pair[continent].loc[i, 'pair_0']==dict_cor_goals_continents_2_condition[continent].loc[j, 'pair_0'] and dict_cor_goals_continents_2_pair[continent].loc[i, 'pair_1']==dict_cor_goals_continents_2_condition[continent].loc[j, 'pair_1']:\n",
    "                dict_cor_goals_continents_2[continent].loc[i, 'pair_0'] = dict_cor_goals_continents_2_pair[continent].loc[i, 'pair_0']\n",
    "                dict_cor_goals_continents_2[continent].loc[i, 'pair_1'] = dict_cor_goals_continents_2_pair[continent].loc[i, 'pair_1']\n",
    "                dict_cor_goals_continents_2[continent].loc[i, 'min_dcor'] = min(dict_cor_goals_continents_2_pair[continent].loc[i, 'min_dcor_pair'], dict_cor_goals_continents_2_condition[continent].loc[j, 'min_dcor_cond'])\n",
    "                if dict_cor_goals_continents_2_pair[continent].loc[i, 'min_dcor_pair'] < dict_cor_goals_continents_2_condition[continent].loc[j, 'min_dcor_cond']:\n",
    "                    dict_cor_goals_continents_2[continent].loc[i, 'condition'] = 0\n",
    "                else:\n",
    "                    dict_cor_goals_continents_2[continent].loc[i, 'condition'] = dict_cor_goals_continents_2_condition[continent].loc[j, 'condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dict_cor_goals_continents_2['World']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "dict_cor_goals_continents_2 = pickle.load(open('distance_cor/goals/dict_cor_goals_continents_2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for statistical significance\n",
    "We calculate the p-values of our partial distance correlations, i.e., the probability that the null hypothesis of (partial) independence can be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for continent in continents:\n",
    "    print(continent)\n",
    "    dict_cor_goals_continents_2[continent]['p-value'] = -1\n",
    "    for r, row in dict_cor_goals_continents_2[continent].iterrows():\n",
    "        \n",
    "        # preparing pair_0 and pair_1\n",
    "        if row.pair_1=='T':\n",
    "            row.pair_1 = 18\n",
    "        pair_0_array = continents_prep_g[continent][int(row.pair_0)-1]\n",
    "        pair_1_array = continents_prep_g[continent][int(row.pair_1)-1]\n",
    "        \n",
    "        # extracting conditional variables from column 'condition'\n",
    "        cond_list = []\n",
    "        for i in row.condition.split():\n",
    "            newstr = ''.join((ch if ch in '0123456789.-eT' else ' ') for ch in i)\n",
    "            cond_list.extend([i for i in newstr.split()])\n",
    "\n",
    "        condition = []\n",
    "        for c in cond_list:\n",
    "            if c=='T':\n",
    "                condition.extend(continents_prep_g[continent][17].T)\n",
    "            else:\n",
    "                condition.extend(continents_prep_g[continent][int(c)-1].T)\n",
    "\n",
    "        cond_array = np.asarray(condition).T\n",
    "    \n",
    "        dict_cor_goals_continents_2[continent].iloc[r, 4] = dcor.independence.partial_distance_covariance_test(pair_0_array, pair_1_array, cond_array, num_resamples=10000).p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with alpha = 0.1\n",
    "dict_cor_goals_continents_2['Eastern Africa'][dict_cor_goals_continents_2['Eastern Africa']['p-value'] < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "g_cor = open('distance_cor/goals/dict_cor_goals_continents_2.pkl', 'wb')\n",
    "pickle.dump(dict_cor_goals_continents_2, g_cor)\n",
    "g_cor.close()\n",
    "\n",
    "# saving as csv's\n",
    "for continent in continents:\n",
    "    dict_cor_goals_continents_2[continent] = dict_cor_goals_continents_2[continent][['pair_0', 'pair_1', 'min_dcor', 'p-value', 'condition']]\n",
    "    dict_cor_goals_continents_2[continent]['p-value'] = dict_cor_goals_continents_2[continent]['p-value'].astype(float).round(5)\n",
    "    dict_cor_goals_continents_2[continent].min_dcor = np.sqrt(dict_cor_goals_continents_2[continent].min_dcor.astype(float)).round(5)\n",
    "    dict_cor_goals_continents_2[continent].to_csv('distance_cor/goals/conditions_{}.csv'.format(continent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep the minimum significant distance correlation of each pair of two goals, pairwise or conditioned on any potential subset.\n",
    "\n",
    "The last step is to insert these values into the right cell in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_goals_continents_2 = {}\n",
    "\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    cor_goals_continents_2[continent] = pd.DataFrame(index=goals, columns=goals)\n",
    "\n",
    "    for i in list(dict_cor_goals_continents_2[continent].index):\n",
    "        goal_0 = dict_cor_goals_continents_2[continent].loc[i, 'pair_0']\n",
    "        goal_1 = dict_cor_goals_continents_2[continent].loc[i, 'pair_1']\n",
    "        \n",
    "        # take square root because we have previously squared the distance correlation\n",
    "        cor_goals_continents_2[continent].loc[goal_1, goal_0] = np.sqrt(dict_cor_goals_continents_2[continent].loc[i, 'min_dcor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check\n",
    "cor_goals_continents_2['Africa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cor_goals_continents_2` are the conditional distance correlations for all continents in a setting of 18 random vectors $X$, $Y$, and $Z_1, Z_2, ..., Z_{16}$, where $\\boldsymbol{Z}$ is the array containing all random vectors we want to condition on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "g_cor = open('distance_cor/goals/dcor_goals_continents_2.pkl', 'wb')\n",
    "pickle.dump(cor_goals_continents_2, g_cor)\n",
    "g_cor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation on goal-level\n",
    "Additionally to the matrices with numbers, we would also like to visualise these matrices and plot these correlations as networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continents\n",
    "for continent in continents:\n",
    "    # generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(cor_goals_continents_2[continent].fillna(0), dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(25, 22))\n",
    "\n",
    "    # generate a custom diverging colormap\n",
    "    cmap = sns.color_palette(\"Reds\", 100)\n",
    "\n",
    "    # draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(cor_goals_continents_2[continent].fillna(0), mask=mask, cmap=cmap, vmax=1, center=0.5, vmin=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('{}'.format(continent), fontdict={'fontsize': 52})\n",
    "    plt.savefig('distance_cor/goals/{}_cor_goals.png'.format(continent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation for networkX\n",
    "dcor_dict_g = {}\n",
    "\n",
    "for continent in cor_goals_continents_2.keys():\n",
    "    dcor_dict_g[continent] = {}\n",
    "\n",
    "    for goalcombination in g_combinations:\n",
    "        dcor_dict_g[continent][tuple(goalcombination)] = [cor_goals_continents_2[continent].loc[goalcombination[1], goalcombination[0]], float(dict_cor_goals_continents_2[continent].loc[(dict_cor_goals_continents_2[continent]['pair_0']=='{}'.format(goalcombination[0])) & (dict_cor_goals_continents_2[continent]['pair_1']=='{}'.format(goalcombination[1]))]['p-value'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for continent in cor_goals_continents_2.keys():\n",
    "    for key in dcor_dict_g[continent].keys():\n",
    "        if key[1] == 'T':\n",
    "            dcor_dict_g[continent][tuple((key[0], '18'))] = dcor_dict_g[continent].pop(tuple((key[0], 'T')))\n",
    "        elif key[0] == 'T':\n",
    "            dcor_dict_g[continent][tuple(('18', key[1]))] = dcor_dict_g[continent].pop(tuple(('T', key[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palettes to choose from\n",
    "'PuBu'\n",
    "'YlOrRd'\n",
    "'Reds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting networks with weighted edges\n",
    "\n",
    "layout = 'circular'\n",
    "\n",
    "centrality_C = {}     # dictionary to save centralities\n",
    "degree_C = {}    # dictionary to save degrees\n",
    "density_C = {}    # dictionary to save weighted densities\n",
    "p_C = {}    # auxiliary\n",
    "partition_C = {}    # dictionary to save clusters\n",
    "\n",
    "for continent in cor_goals_continents_2.keys():\n",
    "    G_C = nx.Graph()\n",
    "\n",
    "    for key, value in dcor_dict_g[continent].items():\n",
    "        if value[1] <= 0.01:\n",
    "            w = value[0]\n",
    "            s = 'solid'\n",
    "            #c = sns.color_palette('YlOrRd', 3)[2]\n",
    "            c = sns.color_palette('Reds', 100)[int(value[0]*100)]\n",
    "        elif 0.01 < value[1] <= 0.05:\n",
    "            w = value[0]\n",
    "            s = 'dashed'\n",
    "            #c = sns.color_palette('YlOrRd', 3)[1]\n",
    "            c = sns.color_palette('Reds', 100)[int(value[0]*100)]\n",
    "        elif 0.05 < value[1] <= 0.1:\n",
    "            w = value[0]\n",
    "            s = 'dotted'\n",
    "            #c = sns.color_palette('YlOrRd', 3)[0]\n",
    "            c = sns.color_palette('Reds', 100)[int(value[0]*100)]\n",
    "        else:\n",
    "            w = 0\n",
    "            s = 'solid'\n",
    "            c = 'white'\n",
    "        G_C.add_edge(int(key[0]), int(key[1]), style=s, weight=w, color=c, alpha=value[0])\n",
    "        \n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G_C)\n",
    "    elif layout == 'spring':\n",
    "        pos = nx.spring_layout(G_C)\n",
    "    \n",
    "    plt.figure(figsize=(24,16))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G_C, pos, node_size=1000)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G_C, pos, font_size=46, font_family='sans-serif')\n",
    "\n",
    "    nodes = G_C.nodes()\n",
    "    edges = G_C.edges()\n",
    "    colors = [G_C[u][v]['color'] for u,v in edges]\n",
    "    weights = [G_C[u][v]['weight'] for u,v in edges]\n",
    "    alphas = [G_C[u][v]['alpha'] for u,v in edges]\n",
    "    styles = [G_C[u][v]['style'] for u,v in edges]\n",
    "\n",
    "    nx.draw_networkx_nodes(G_C, pos, nodelist=nodes, node_color='white', node_size=1000)\n",
    "\n",
    "    for i, edge in enumerate(edges):\n",
    "        pos_edge = {edge[0]: pos[edge[0]], edge[1]: pos[edge[1]]}\n",
    "        nx.draw_networkx_edges(G_C, pos_edge, edgelist=[edge], edge_color=colors[i], style=styles[i], width=np.multiply(weights[i],25)) #alpha=np.multiply(alphas[i],2.5))\n",
    "        \n",
    "    #nx.draw_networkx(G_C, pos, with_labels=False, edges=edges, edge_color=colors, node_color='white', node_size=1000, width=np.multiply(weights,25))\n",
    "    \n",
    "    ax=plt.gca()\n",
    "    fig=plt.gcf()\n",
    "    trans = ax.transData.transform\n",
    "    trans_axes = fig.transFigure.inverted().transform\n",
    "    imsize = 0.08    # this is the image size\n",
    "    plt.title('{}'.format(continent), y=1.05, fontdict={'fontsize': 52})\n",
    "\n",
    "    for node in G_C.nodes():\n",
    "        (x,y) = pos[node]   \n",
    "        xx,yy = trans((x,y)) # figure coordinates\n",
    "        xa,ya = trans_axes((xx,yy)) # axes coordinates\n",
    "        a = plt.axes([xa-imsize/2.0,ya-imsize/2.0, imsize, imsize])\n",
    "        a.imshow(mpimg.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(node)))\n",
    "        a.axis('off')\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_{}_network_logos.png'.format(continent, layout), format='png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # weighted centrality\n",
    "    centr = nx.eigenvector_centrality(G_C, weight='weight', max_iter=100000)\n",
    "    centrality_C[continent] = sorted((v, '{:0.2f}'.format(c)) for v, c in centr.items())\n",
    "    \n",
    "    degree_C[continent] = dict(G_C.degree(weight='weight'))\n",
    "    \n",
    "    # weighted density\n",
    "    density_C[continent] = 2 * np.sum(weights) / (len(nodes) * (len(nodes) - 1))\n",
    "    \n",
    "    # weighted clustering with Louvain algorithm\n",
    "    part_C = {}\n",
    "    modularity_C = {}\n",
    "    for i in range(100):\n",
    "        part_C[i] = community.best_partition(G_C, random_state=i)\n",
    "        modularity_C[i] = community.modularity(part_C[i], G_C)\n",
    "    \n",
    "    p_C[continent] = part_C[max(modularity_C, key=modularity_C.get)]\n",
    "\n",
    "    # having lists with nodes being in different clusters\n",
    "    partition_C[continent] = {}\n",
    "    for com in set(p_C[continent].values()) :\n",
    "        partition_C[continent][com] = [nodes for nodes in p_C[continent].keys() if p_C[continent][nodes] == com]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clusters\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(partition_C[continent])\n",
    "    print('-------------------------')\n",
    "\n",
    "g_part = open('distance_cor/goals/partition_continents.pkl', 'wb')\n",
    "pickle.dump(partition_C, g_part)\n",
    "g_part.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centralities\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(centrality_C[continent])\n",
    "    print('-------------------------')\n",
    "\n",
    "g_cent = open('distance_cor/goals/centrality_continents.pkl', 'wb')\n",
    "pickle.dump(centrality_C, g_cent)\n",
    "g_cent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(degree_C[continent])\n",
    "    print('-------------------------')\n",
    "\n",
    "g_deg = open('distance_cor/goals/degree_continents.pkl', 'wb')\n",
    "pickle.dump(degree_C, g_deg)\n",
    "g_deg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# densities\n",
    "for continent in continents:\n",
    "    print(continent)\n",
    "    print(density_C[continent])\n",
    "    print('-------------------------')\n",
    "    \n",
    "g_dens = open('distance_cor/goals/density_continents.pkl', 'wb')\n",
    "pickle.dump(degree_C, g_dens)\n",
    "g_dens.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(goal):\n",
    "    return OffsetImage(plt.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(goal)), zoom=0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for continent in cor_goals_continents_2.keys():\n",
    "    # separating goals from their centralities\n",
    "    x = []\n",
    "    y = []\n",
    "    for cent in centrality_C[continent]:\n",
    "        x.append(cent[0])\n",
    "        y.append(float(cent[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(24,16))\n",
    "    #plt.tight_layout()\n",
    "    plt.title('{}'.format(continent), y=1.05, fontdict={'fontsize': 52})\n",
    "    ax.scatter(x, y) \n",
    "    \n",
    "    # adding images\n",
    "    for x0, y0, goal in zip(x, y, list(nodes)):\n",
    "        ab = AnnotationBbox(get_image(goal), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticklabels([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], fontsize=28)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_ylim(0, 0.75)\n",
    "    ax.set_ylabel('Eigenvector centrality', labelpad=24, fontdict={'fontsize': 38})\n",
    "    ax.set_xlabel('Variables (SDGs + climate change)', labelpad=54, fontdict={'fontsize': 38})\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_eigenvector_centrality.png'.format(continent), format='png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting clusters in networks with weighted edges\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "layout = 'spring'\n",
    "\n",
    "for continent in cor_goals_continents_2.keys():\n",
    "    G_C = nx.Graph()\n",
    "\n",
    "    for key, value in dcor_dict_g[continent].items():\n",
    "        G_C.add_edge(int(key[0]), int(key[1]), weight=value, color=sns.color_palette(\"Reds\", 100)[int(np.around(value*100))], alpha=value)\n",
    "        \n",
    "    if layout == 'circular':\n",
    "        pos = nx.circular_layout(G_C)\n",
    "    elif layout == 'spring':\n",
    "        pos = nx.spring_layout(G_C, iterations=100, seed=42)\n",
    "    \n",
    "    plt.figure(figsize=(24,16))\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G_C, pos, node_size=1000)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G_C, pos, font_size=46, font_family='sans-serif')\n",
    "\n",
    "    nodes = G_C.nodes()\n",
    "    edges = G_C.edges()\n",
    "    colors = [G_C[u][v]['color'] for u,v in edges]\n",
    "    weights = [G_C[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "    nx.draw_networkx(G_C, pos, with_labels=False, edges=edges, edge_color=colors, node_color='white', node_size=1000, width=np.multiply(weights,20))\n",
    "\n",
    "    ax=plt.gca()\n",
    "    fig=plt.gcf()\n",
    "    trans = ax.transData.transform\n",
    "    trans_axes = fig.transFigure.inverted().transform\n",
    "    imsize = 0.08    # this is the image size\n",
    "    plt.title('{}'.format(continent), y=1.05, fontdict={'fontsize': 52})\n",
    "\n",
    "    for node in G_C.nodes():\n",
    "        x,y = pos[node]   \n",
    "        xx,yy = trans((x,y)) # figure coordinates\n",
    "        xa,ya = trans_axes((xx,yy)) # axes coordinates\n",
    "        a = plt.axes([xa-imsize/2.0,ya-imsize/2.0, imsize, imsize])\n",
    "        a.imshow(mpimg.imread('utils/images/E_SDG goals_icons-individual-rgb-{}.png'.format(node)))\n",
    "        a.axis('off')\n",
    "    \n",
    "    # finding clusters with maximum modularity\n",
    "    clusters = []\n",
    "    for com, goals in partition_C[continent].items():\n",
    "        position = []\n",
    "        for goal in goals:\n",
    "            x,y = pos[goal]\n",
    "            position.append((x,y))\n",
    "        \n",
    "        positions = []\n",
    "        for i in range(6000):\n",
    "            np.random.shuffle(position)\n",
    "            positions.extend(position)\n",
    "        \n",
    "        # polygens\n",
    "        polygon = Polygon(positions, closed=False)\n",
    "        clusters.append(polygon)\n",
    "    \n",
    "    np.random.seed(72)\n",
    "    colors = 100*np.random.rand(len(clusters))\n",
    "    p = PatchCollection(clusters, alpha=0.4)\n",
    "    p.set_array(np.array(colors))\n",
    "    ax.add_collection(p)\n",
    "        \n",
    "    plt.axis('off')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.savefig('distance_cor/goals/{}_{}_network_logos_cluster.png'.format(continent, layout), format='png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
