{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for the SDG Indicators by (1) UN and (2) WorldBank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) UN data set\n",
    "\n",
    "**We use UN SDG's data set and convert this data set, so every country, continent, etc. is in a separate <code>csv</code> file.**\n",
    "\n",
    "To get started, we download the entire available data from https://unstats.un.org/sdgs/indicators/database/ and call it <code>un_data.csv</code>.\n",
    "\n",
    "\n",
    "Let's load the data set and look at its columns and rows to figure out how it is structured.\n",
    "\n",
    "\n",
    "**We aim to have one pandas data frame per country, with all indicators. We save them as separate <code>csv</code> files.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the usual imports and loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data set\n",
    "all_data = pd.read_csv('utils/data/data.csv', dtype=object)\n",
    "# the percentage of targets we have data far\n",
    "print(round(len(all_data.Target.unique())/169, 3)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN data for SDG 13\n",
    "SDG13_data = pd.read_csv('utils/SDG13_data.csv', dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "SDG13_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out data not belonging to SDG 13\n",
    "sdg13_data = SDG13_data[SDG13_data['Goal']=='13']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is structured by indicators and years in rows in one large data frame with all countries. We would like to have one data frame per country. Hence, we first extract the names of *regional groupings*, i.e. countries, continents, etc., and the names of so-called *other groupings*.\n",
    "\n",
    "According to the UN Statistics Division, other groupings include Least Developed Countries (LDC), Land Locked Developing Countries (LLDC), Small Island Developing States (SIDS), Developed Regions, and Developing Regions. \n",
    "\n",
    "Developing Regions are Latin America and the Caribbean, South-Eastern Asia, Southern Asia, Southern Asia (excluding India), Caucasus and Central Asia, Eastern Asia (excluding Japan and China), Western Asia (exc. Armenia, Azerbaijan, Cyprus, Israel and Georgia), Eastern Asia (excluding Japan), Oceania (exc. Australia and New Zealand), Sub-Saharan Africa (inc. Sudan), and Northern Africa (exc. Sudan).\n",
    "\n",
    "**All these groupings can be subject to separate network analyses of the indicators later on.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's first see all different columns of our data frame before we only see these different groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have even lots of information on a sub-indicator level and this might be subject to more detailed analyses later on. We could, e.g., indicator 4.6.1* explore by disparate age goups and by sex.\n",
    "\n",
    "\\* *Indicator 4.6.1: Proportion of population in a given age group achieving at least a fixed level of proficiency in functional (a) literacy and (b) numeracy skills, by sex.*\n",
    "\n",
    "\n",
    "We keep this possibility open, but now, let's not go further into a sub-indicator level and see the different groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings_UN = list(SDG13_data['GeoAreaName'].unique())\n",
    "groupings_UN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data.replace({\"Democratic People's Republic of Korea\": \"Korea, Dem. People's Rep.\", 'Gambia': 'Gambia, The', 'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom', 'Congo': 'Congo, Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.', 'Czechia': 'Czech Republic', 'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'Kyrgyzstan': 'Kyrgyz Republic', \"Lao People's Democratic Republic\": 'Lao PDR', 'Republic of Moldova': 'Moldova', 'Micronesia (Federated States of)': 'Micronesia, Fed. Sts.', 'Slovakia': 'Slovak Republic', 'Viet Nam': 'Vietnam', 'Egypt': 'Egypt, Arab Rep.', 'United Republic of Tanzania': 'Tanzania','United States of America': 'United States', 'Venezuela (Bolivarian Republic of)': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.', 'Bahamas': 'Bahamas, The', 'Bolivia (Plurinational State of)': 'Bolivia'}, inplace=True)\n",
    "sdg13_data.replace({\"Republic of Korea\": \"Korea, Rep.\", \"Democratic People's Republic of Korea\": \"Korea, Dem. People's Rep.\", 'Gambia': 'Gambia, The', 'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom', 'Congo': 'Congo, Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.', 'Czechia': 'Czech Republic', 'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'Kyrgyzstan': 'Kyrgyz Republic', \"Lao People's Democratic Republic\": 'Lao PDR', 'Republic of Moldova': 'Moldova', 'Micronesia (Federated States of)': 'Micronesia, Fed. Sts.', 'Slovakia': 'Slovak Republic', 'Viet Nam': 'Vietnam', 'Egypt': 'Egypt, Arab Rep.', 'United Republic of Tanzania': 'Tanzania','United States of America': 'United States', 'Venezuela (Bolivarian Republic of)': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.', 'Bahamas': 'Bahamas, The', 'Bolivia (Plurinational State of)': 'Bolivia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of keys to delete\n",
    "delete_groups = []\n",
    "\n",
    "for g in list(groupings):\n",
    "    if g not in countries:\n",
    "        delete_groups.append(g)\n",
    "        \n",
    "# delete\n",
    "for dg in delete_groups:\n",
    "    groupings.remove(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take World Bank countries\n",
    "c = pd.read_csv('utils/countries_wb.csv', dtype=str, delimiter=';', header=None)\n",
    "countries = list(c[0])\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading World Bankd data set\n",
    "wb_data = pd.read_csv('utils/data/wb_data.csv', dtype=object)\n",
    "wb_data.drop(wb_data.tail(5).index,inplace=True)    # 5 last rows are blank / have other info\n",
    "wb_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(wb_data.columns)\n",
    "for column in columns[4:]:\n",
    "    columns.append(column[:4])\n",
    "    columns.remove(column)\n",
    "\n",
    "wb_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = columns[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now save a data frame with all of the meta-information. We delete the columns which are specific in area and time, and of course we do not want to have the values in this data frame. In the end, we delete all duplicate entries in the column **SeriesCode**. So, we are left with the information we wanted: mapping the series codes to the indicators, the Source for the data, the Units measured in, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = sdg13_data.drop(columns=['GeoAreaCode', 'GeoAreaName', 'TimePeriod', 'Value', 'Time_Detail']).drop_duplicates(subset=['Indicator', 'SeriesCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all series codes of SDG 13\n",
    "seriescodes_13 = set(list(info['SeriesCode']))\n",
    "seriescodes_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many we have\n",
    "len(seriescodes_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the data set into multiple small data sets by creating a dictionary that contains the groupings' names as keys. \n",
    "\n",
    "First, we create empty data frames for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_all = {c: pd.DataFrame() for c in countries}\n",
    "dict_13 = {c: pd.DataFrame() for c in countries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, should be empty\n",
    "#dict_all.get('Belize')\n",
    "dict_13.get('Belize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we replace each of the empty data frames with the data we have available for them. Note, that our dictionary will be the ensamble of all groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:    # memory-intensive\n",
    "    #dict_all[c] = all_data[all_data['GeoAreaName'].isin(['{}'.format(c)])]\n",
    "    dict_13[c] = sdg13_data[sdg13_data['GeoAreaName'].isin(['{}'.format(c)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "dict_13['Azerbaijan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have one data frame per country. The next step is to have years as columns.\n",
    "\n",
    "The next cell gives us the series codes in the rows and the years in the columns. These series codes are unique descriptions of the sub-indicators and we match these series codes to indicators and all other information in a different data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    #dict_all[c] = dict_all.get(c).pivot_table(values='Value', index='SeriesCode', columns='TimePeriod', dropna=False, aggfunc='first')\n",
    "    if c not in groupings_UN:\n",
    "        dict_13[c] = pd.DataFrame(index=seriescodes_13, columns=years)\n",
    "    else:\n",
    "        dict_13[c] = dict_13.get(c).pivot_table(values='Value', index='SeriesCode', columns='TimePeriod', dropna=False, aggfunc='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dict_13['Austria']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up and transforming all country data frames into the same dimensions\n",
    "\n",
    "We have a couple of things to do to make our data frames workable:\n",
    "1. We have some values in the data frames which we do not want, as e.g. <code>,</code>, <code> = </code>, <code>N</code>, etc. We replace them with appropriate values, i.e. <code>0</code>, or simply a space. \n",
    "2. Some data frames have data from **1990** to **2018**, some others from **1992** to **2018**. We want to have all data frames having data from **1990** to **2018**, i.e. an equal amount of columns. The additional columns are filled with <code>NaNs</code>.\n",
    "3. Some data frames have not all indicators and sub-indicators listed, but we would like to have all of them in all data frames. These additional rows are filled with <code>NaNs</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the first task, i.e. cleaning up the data frames.\n",
    "\n",
    "We first need to define lists for all years, i.e. **1990** to **2018** and all indicators and sub-indicators, i.e. series codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change <span style=\"color:red\"> 'Haiti' </span> in the cell below to a few other countries and you'll see that they can have different lengths. We need to bring all on the same length. We agree on having data for the **years 1990 to 2019**.\n",
    "\n",
    "Now, we insert the missing years for all groupings. We want to have NaNs in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "list(dict_13['Germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we insert the missing years as columns for all groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:    # memory-intensive\n",
    "    for year in years:\n",
    "        if year not in list(dict_13[c]):\n",
    "            dict_13[c]['{}'.format(year)] = np.nan\n",
    "    # having the years in order\n",
    "    dict_13[c] = dict_13[c][years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "dict_13['Germany']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we insert the missing series codes as rows.\n",
    "\n",
    "Let's first see how many rows do we have for <span style=\"color:red\"> Guam </span>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict_13['Nicaragua'].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have all $J$ sub-indicators we want for each country as rows. We fill these rows with NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    for seriescode in seriescodes_13:\n",
    "        if seriescode not in list(dict_13[c].index):\n",
    "            dict_13[c].loc[seriescode] = np.nan    # fill these rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: do we have J many?\n",
    "len(list(dict_13['Nicaragua'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert all to floats\n",
    "for c in countries:\n",
    "    for year in years:    \n",
    "        for seriescode in seriescodes_13:\n",
    "            if not isinstance(dict_13[c].loc[seriescode, year], float):\n",
    "                dict_13[c].loc[seriescode, year] = float(dict_13[c].loc[seriescode, year].replace(',', '').replace('<', '').replace('>', '').replace('=', '').replace('N', '0').replace(' -   ', '0').replace('0V', '0').replace('. . .', '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check: are all series codes as rows?\n",
    "len(list(dict_13['Nicaragua'].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save all countries as different <code>csv</code> files and as one `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('csv_original'):\n",
    "    os.mkdir('csv_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in countries:\n",
    "    dict_all[c].to_csv(r'csv_original/{}.csv'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the information file might also be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('utils'):\n",
    "    os.mkdir('utils')\n",
    "    \n",
    "info.to_csv(r'utils/info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as one pickle file\n",
    "dict13 = open('utils/data/dict_13.pkl', 'wb')\n",
    "pickle.dump(dict_13, dict13)\n",
    "dict13.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as one pickle file\n",
    "dictall = open('utils/data/dict_all.pkl', 'wb')\n",
    "pickle.dump(dict_all, dictall)\n",
    "dictall.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "dictall = open('utils/data/dict_all.pkl', 'rb')\n",
    "dict_all = pickle.load(dictall)\n",
    "dictall.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising time-series\n",
    "\n",
    "We quickly visualise the time-series to get a better idea of the characteristics of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax2.plot(list(range(2000, 2020)), dict_all['Bolivia'].loc['DC_ODA_BDVL'], color='#42B24C', linewidth=5)\n",
    "ax.plot(list(range(2000, 2020)), dict_all['Bolivia'].loc['DC_TRF_TOTL'], color='#DE0E68', linewidth=5)\n",
    "\n",
    "ax2.set_ylim(0, 251)  # biodiversity ODA\n",
    "ax.set_ylim(250, 2501)  # total ODA\n",
    "\n",
    "# hide the spines between ax and ax2\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax.xaxis.tick_top()\n",
    "ax.tick_params(labelsize=20, labeltop='off')  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "plt.xticks(np.arange(2000, 2019, step=2), size=20)\n",
    "ax2.tick_params(labelsize=20)\n",
    "\n",
    "f.set_figheight(8)\n",
    "f.set_figwidth(12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(list(range(2000, 2020)), dict_all['Bolivia'].loc['DC_ODA_BDVL'], color='#42B24C', linewidth=5)\n",
    "plt.xticks(np.arange(2000, 2019, step=2), size=20)\n",
    "plt.yticks(np.arange(0, 251, step=25), size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(list(range(2000, 2020)), dict_all['Bolivia'].loc['DC_TRF_TOTL'], color='#DE0E68', linewidth=5)\n",
    "plt.xticks(np.arange(2000, 2019, step=2), size=20)\n",
    "plt.yticks(np.arange(0, 2501, step=250), size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.yticks(np.arange(0, 2001, step=250), size=20)\n",
    "plt.xticks(np.arange(0, 251, step=25), size=20)\n",
    "plt.plot(dict_all['Bolivia'].loc['DC_ODA_BDVL'], dict_all['Bolivia'].loc['DC_TRF_TOTL'], '--bo'); #, s=100, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardisation\n",
    "We have saved the original data set, but it is often useful to have the data standardised. Standardising a data set involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1. Standardisation is often required by machine learning algorithms when your time series data has input values with differing scales. \n",
    "\n",
    "We create a new dictionary `dict_all_std` to keep the standardised values separate to the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT (we don't want to re-run the entire script every time we continue working on it)\n",
    "dict_all = pickle.load(open('utils/data/dict_all.pkl', 'rb'))\n",
    "dict_all_std = pickle.load(open('utils/data/dict_all_std.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~15 minutes computing time\n",
    "#dict_all_std = copy.deepcopy(dict_all)\n",
    "dict_13_std = copy.deepcopy(dict_13)   \n",
    "\n",
    "for country in countries:\n",
    "    for seriescode in seriescodes_13:\n",
    "        dict_13_std[country].loc[seriescode] = scale(dict_13[country].loc[seriescode])\n",
    "        #dict_all_std[country].loc[seriescode] = scale(dict_all[country].loc[seriescode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check\n",
    "print('Original value', dict_13['France'].loc['VC_DSR_DAFF'])\n",
    "print('-------')\n",
    "print('Standardised value', dict_13_std['France'].loc['VC_DSR_DAFF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We better save `dict_all_std`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as csv files per grouping\n",
    "if not os.path.exists('csv_standardised'):\n",
    "    os.mkdir('csv_standardised')\n",
    "    \n",
    "for group in groupings:\n",
    "    dict_all_std[group].to_csv(r'csv_standardised/{}.csv'.format(group))\n",
    "\n",
    "# as one pickle file\n",
    "stand = open('utils/data/dict_all_std.pkl', 'wb')\n",
    "pickle.dump(dict_all_std, stand)\n",
    "stand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) World Bank data set\n",
    "\n",
    "**We use World Bank's data set and convert this data set, so every country, continent, etc. is in a separate <code>csv</code> file.**\n",
    "\n",
    "To get started, we download the entire available data from http://datatopics.worldbank.org/sdgs/ and call it <code>wb_data.csv</code>.\n",
    "\n",
    "\n",
    "Let's load the data set and look at its columns and rows to figure out how it is structured.\n",
    "\n",
    "\n",
    "**We aim to have one pandas data frame per country, with all indicators. We save them as separate <code>csv</code> files.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute when new indicators are added -> call new metadata file wb_info_new.csv\n",
    "\n",
    "wb_info_new = pd.read_csv('utils/wb_info.csv', header=None, dtype=object)\n",
    "print(len(wb_info_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the percentage of targets we have data far\n",
    "print(round(len(wb_info_new[4].unique())/169, 4)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are any indicators double?\n",
    "wb_info_new[wb_info_new.duplicated(subset=[2])==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop this indicator\n",
    "wb_info_new.drop_duplicates(subset=[2], inplace=True)\n",
    "print(len(wb_info_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for code in wb_info_new[0]:\n",
    "    if code not in list(wb_info[0]):\n",
    "        print(code)\n",
    "        i += 1\n",
    "\n",
    "print()\n",
    "print('# added indicators:', i)\n",
    "print()\n",
    "\n",
    "j = 0\n",
    "\n",
    "for code in wb_info[0]:\n",
    "    if code not in list(wb_info_new[0]):\n",
    "        print(code)\n",
    "        j += 1\n",
    "\n",
    "print()\n",
    "print('# deleted indicators:', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = list(wb_data['Country Name'].unique())\n",
    "# save countries\n",
    "np.savetxt('utils/countries_wb_all.csv', all_countries, delimiter=';', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all_wb = {country: pd.DataFrame() for country in countries}\n",
    "for country in countries:\n",
    "    print(country)\n",
    "    dict_all_wb[country] = wb_data[wb_data['Country Name'].isin(['{}'.format(country)])]\n",
    "    dict_all_wb[country] = dict_all_wb[country].drop(columns=['Country Name', 'Country Code', 'Series Name'])\n",
    "    dict_all_wb[country].set_index('Series Code', inplace=True)\n",
    "    dict_all_wb[country] = pd.concat([dict_all_wb[country], dict_13[country]])    # adding series codes for SDG 13\n",
    "    dict_all_wb[country] = dict_all_wb[country].replace('..', np.nan).astype(float)\n",
    "    dict_all_wb[country] = dict_all_wb[country].drop(index='DT.ODA.ODAT.CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all_wb['Korea, Rep.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seriescodes_wb = set(list(dict_all_wb['Germany'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data\n",
    "for country in countries: \n",
    "    dict_all_wb[country].to_csv(r'csv_original/{}_wb.csv'.format(country))\n",
    "    \n",
    "# as one pickle file\n",
    "dictall = open('utils/data/dict_all_wb.pkl', 'wb')\n",
    "pickle.dump(dict_all_wb, dictall)\n",
    "dictall.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all_wb_std = copy.deepcopy(dict_all_wb)\n",
    "\n",
    "for country in countries:\n",
    "    for seriescode in seriescodes_wb:\n",
    "        # adding noise as representative for measurement errors\n",
    "        #noise = np.random.normal(scale=0.1, size=len(dict_all_wb[country].loc[seriescode]))\n",
    "        \n",
    "        #dict_all_wb[country].loc[seriescode] = dict_all_wb[country].loc[seriescode] + noise\n",
    "        \n",
    "        dict_all_wb_std[country].loc[seriescode] = scale(dict_all_wb[country].loc[seriescode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "print('Original value', dict_all_wb['Belgium'].loc['ER.H2O.FWTL.ZS'])\n",
    "print('-------')\n",
    "print('Standardised value', dict_all_wb_std['Belgium'].loc['ER.H2O.FWTL.ZS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We better save `dict_all_wb_std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as csv files per grouping\n",
    "if not os.path.exists('csv_standardised'):\n",
    "    os.mkdir('csv_standardised')\n",
    "    \n",
    "for country in countries:\n",
    "    dict_all_wb_std[country].to_csv(r'csv_standardised/{}_wb.csv'.format(country))\n",
    "\n",
    "# as one pickle file\n",
    "stand = open('utils/data/dict_all_wb_std.pkl', 'wb')\n",
    "pickle.dump(dict_all_wb_std, stand)\n",
    "stand.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
